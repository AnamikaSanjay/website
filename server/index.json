[{
    "section": "",
    "url": "https://intelops.ai/about/",
    "title": "About",
    "description": "Get fastest loan with smart way a seating home",
    "searchKeyword": "",
    "content": ""
  },{
    "section": "",
    "url": "https://intelops.ai/contact/",
    "title": "Contact",
    "description": "Contact us to schedule a demo about our products and services.",
    "searchKeyword": "contact, contact us",
    "content": ""
  },{
    "section": "",
    "url": "https://intelops.ai/features/",
    "title": "Features",
    "description": "Libero consequuntur doloremque amet, cum fugiat ipsam blanditiis corrupti praesentium quis.",
    "searchKeyword": "",
    "content": ""
  },{
    "section": "",
    "url": "https://intelops.ai/how-it-works/",
    "title": "How It Works",
    "description": "Libero consequuntur doloremque amet, cum fugiat ipsam blanditiis corrupti praesentium quis.",
    "searchKeyword": "",
    "content": ""
  },{
    "section": "",
    "url": "https://intelops.ai/mind-map/",
    "title": "Mind Map",
    "description": "Hierarchical arrangement of order of things.",
    "searchKeyword": "",
    "content": ""
  },{
    "section": "",
    "url": "https://intelops.ai/team/",
    "title": "Our Team",
    "description": "Some of our Team members",
    "searchKeyword": "",
    "content": "This is the start of dummy description text. You can change it whenever you want. This is the start of dummy description text. You can change it whenever you want. This is the start of dummy description text. You can change it whenever you want. This is the start of dummy description text. You can change it whenever you want. This is the start of dummy description text. You can change it whenever you want. This is the start of dummy description text. You can change it whenever you want. This is the start of dummy description text. You can change it whenever you want. This is the start of dummy description text. You can change it whenever you want. This is the start of dummy description text. You can change it whenever you want. This is the start of dummy description text. You can change it whenever you want.\n"
  },{
    "section": "Blog",
    "url": "https://intelops.ai/blog/how-to-instrument-nextjs-frontend-application-using-opentelemetry-and-signoz/",
    "title": "Opentelemetry and Signoz",
    "description": "How to instrument a Next.js frontend application using openTelemetry and Signoz",
    "searchKeyword": "",
    "content": " openTelemetry is a open-source project which was hosted by CNCF. It provides a standard way to generate telemetry data that is data like logs, metrics, events and also traces which are all created by your applications. openTelemetry was created by the merger of: You might not see its usefulness when you just started your application with as a set of microservices, but as the usage of your application increases and time comes for you to scale up, keeping track of all the microservices, their bugs, their metrics and so on becomes difficult. This is where openTelemetry comes into play, be it logs, metrics, distributed traces or just traces openTelemetry provides a single standard for observability - store, visualize and query the data with the help of SDKs, APIs and othertools.\n OpenTelemetry in Next.js Deploying A Vector Aggregator Modifying The Values Files Let\u0026rsquo;s start by opening up the collector_values.yaml file we had saved earlier. In this file, we need to change a few configurations for our OTel Collector to be able to communicate with our Vector Aggregator.\n The mode value needs to be changed from empty quotes to \u0026ldquo;deployment\u0026rdquo;. For this particular tutorial, using Pixie as the data source, the config parameter of this file needs to be modified to reflect Pixie as a data receiver: config: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 ` Within the same config parameter, we need to set the exporters. We will be exporting to two places, the logs for the OTel Collector pod, and the HTTP connection to Vector. exporters: logging: loglevel: debug sampling_initial: 5 sampling_thereafter: 200 otlphttp: endpoint: http://vector.vector.svc.cluster.local:80 \rHere, I have decided to use port 80` to send data to Vector. Within the config.service.pipelines parameter, we need to modify the following: pipelines: logs: { } metrics: receivers: [ otlp ] processors: [ memory_limiter, batch ] exporters: [ otlphttp, logging ] traces: receivers: [ otlp ] processors: [ memory_limiter, batch ] exporters: [ otlphttp, logging ] ` Your OTel Collector is ready to be upgraded. Use the following command to force upgrade your collector with the new values file. helm upgrade otel-collector open-telemetry/opentelemetry-collector -f collector_values.yaml \rNow, let's move on to modifying the vector_values.yaml` file to modfiy the configuration for our Vector Aggregator. The role parameter needs to be set to \u0026ldquo;Aggregator\u0026rdquo;. Within the service.ports parameter, we need to expose port 80 for our otlp (OpenTelemetry) data to flow in: ports: - name: otlp-http port: 80 protocol: TCP targetPort: 80 ` In the customConfig parameter, we provide our custom configuration for the Vector Aggregator we are going to deploy. customConfig: api: enabled: true address: 127.0.0.1:8686 playground: true sources: otlp-http: type: http address: 0.0.0.0:80 # this path is automatically added by OpenTelemetry. # this is because we are exporting metrics, so it adds a default path. # The path can be changed/set in the collector_values.yaml file. path: /v1/metrics encoding: text sinks: stdout: encoding: codec: json inputs: - otlp-http target: stdout type: console \rHere, we are receiving input (sources) from our OTel Collector, at localhost:80. This has a path /v1/metricsappended to it by the OTel Collector itself.\rFor sinks (exporters) we are defining one exporter,standard output (stdout). This will take the data from our HTTP` connection, and output it in the form of logs within our Vector Aggregator pod. Now you have configured all that is necessary for Vector to be able to get data from the OTel Collector. We can upgrade our Vector deployment with the new values, using the following command: helm upgrade vector vector/vector --namespace vector --values vector_values.yaml `  Verifying Our Data Collection Using Vector At this point, with both the OTel Collector and the Vector Aggregator deployed, we should start seeing data flowing from one to the other. Run the command below to see how many events Vector has seen:\n# if on Windows, run in Admin PowerShell kubectl -n vector exec -it statefulset/vector -- vector top Which should give you an output similar to this: Horray! We have finally got the data flowing.\nConclusion In this blog, we have explored how to export data from an OpenTelemetry Collector to a Vector Aggregator. This is done using HTTP, as support for OpenTelemetry over gRPC has not been added yet to Vector. We hope that a solution gets created, but until then, feel free to use this workaround.\n"
  },{
    "section": "Blog",
    "url": "https://intelops.ai/blog/advanced-pxl-script-functions/",
    "title": "Advanced PXL Script Functions",
    "description": "Combining all that we have learned into one script.",
    "searchKeyword": "",
    "content": " Over the past couple of weeks, we have been covering the powerful cluster monitoring tool, Pixie. We have covered how to get custom data by writing your own PXL script. We have also covered how to enhance your custom script by using data manipulation functions provided by Pixie. Let\u0026rsquo;s wrap up our coverage of Pixie\u0026rsquo;s custom data collection by diving into an example of an advanced PXL script.\n A Quick Refresher Pixie is a cloud observation tool used for gathering monitoring metrics from your clusters. It provides a Live UI which is a dashboard that can be accessed from anywhere. It also provides users the ability to execute custom scripts to gather custom data.\nThese scripts can be executed from the Live UI or even from the Pixie API. The language these scripts are written in is PXL, which is similar in syntax to Python. In fact, users of the Python library pandas will notice many similarities between PXL and pandas, such as the fact that they both use dataframes (which are like spreadsheets but faster).\nWhat We Covered In The First Blog In the first blog about PXL, we ran the script below.\n# We import px, which is the library we will be using to add extra data to our table. import px # We gather data from the last 5 minutes, from the `process_stats` table, and create a dataframe from it. df = px.DataFrame(table=\u0026#39;process_stats\u0026#39;, start_time=\u0026#39;-5m\u0026#39;) # Below, we are adding extra data to our table, using `context` or `execution_time_functions` df.pod_id = df.ctx[\u0026#39;pod_id\u0026#39;] df.pod_name = px.upid_to_pod_name(df[\u0026#39;upid\u0026#39;]) df.pod_id = px.pod_name_to_pod_id(df[\u0026#39;pod_name\u0026#39;]) df.cmd = df.ctx[\u0026#39;cmdline\u0026#39;] df.pid = df.ctx[\u0026#39;pid\u0026#39;] df.container_name = df.ctx[\u0026#39;container_name\u0026#39;] df.container_id = df.ctx[\u0026#39;container_id\u0026#39;] # We group the dataframe based on certain attributes, and aggregate the data. df = df.groupby([\u0026#39;pid\u0026#39;, \u0026#39;cmd\u0026#39;, \u0026#39;upid\u0026#39;, \u0026#39;container_name\u0026#39;]).agg() # We display the dataframe. px.display(df, \u0026#39;processes_table\u0026#39;) This script covered the following:\n The syntax of a PXL script. Retrieving a table\u0026rsquo;s data (process_stats) for the past 5 minutes. Adding new columns containing context data. Grouping by certain columns and aggregating the data.  What We Covered In Last Week\u0026rsquo;s Blog In last week\u0026rsquo;s blog, we covered some advanced PXL functions used for manipulating data. These included:\n Joining Tables - so we can add data from other tables based on common attributes between the two tables. Dropping/Creating Columns - so we can add more context to our data. Filtering Data - based on columnar values.  This week, let\u0026rsquo;s dive into using all of these at once to create an advanced PXL script that gives us the exact data that we want!\nCreating Our Script Let\u0026rsquo;s set a goal for this script. I would like to get the conn_stats table data and merge it with the network_stats table data. To do this, I would need to merge both tables on as many unique identifiers as possible. In this case, that would constitute the time_ and pod_id columns.\nThere is only one problem though. The network_stats table is an aggregate table. This means that it collects data on an interval basis, and sums up the collected data, so it can be added to the table. This also means that the time_ column in the network_stats table will not always align with the time_ column in the conn_stats table. In fact, in order to align the two, we will need to manipulate the data in the time_ column to match both tables, which we will do later on.\nLet\u0026rsquo;s start our script by setting up the base structure. We will get both of the tables, and display one for now.\nimport px conn_stats_df = px.DataFrame(\u0026#39;conn_stats\u0026#39;, start_time=\u0026#39;-5m\u0026#39;) net_stats_df = px.DataFrame(\u0026#39;network_stats\u0026#39;, start_time=\u0026#39;-5m\u0026#39;) px.display(conn_stats_df, \u0026#39;conn_stats_df\u0026#39;) Setting Up More Variables (For Context/Merging) Now, let\u0026rsquo;s add more variables to our script to get even more data out of each run/call. Let\u0026rsquo;s start by adding the following contextual columns to the conn_stats_df dataframe:\n... conn_stats_df.pod = conn_stats_df.ctx[\u0026#39;pod\u0026#39;] conn_stats_df.pod_id = px.upid_to_pod_id(conn_stats_df.upid) conn_stats_df.container_name = px.upid_to_container_name(conn_stats_df.upid) conn_stats_df.container_id = px.upid_to_container_id(conn_stats_df.upid) conn_stats_df.namespace = px.pod_id_to_namespace(conn_stats_df.pod_id) conn_stats_df.node = px.pod_id_to_node_name(conn_stats_df.pod_id) ... We will also add a column that will help us merge with the network_stats table later:\n... conn_stats_df.time_aligned = px.bin(conn_stats_df.time_, 1000000000) ... Note that the above function is advanced. Here is what is does:\n Creates a new column called time_aligned Bins the original time_ column.  The binning is done in one second increments (equivalent to 1000000000 nanoseconds) You can think of this as \u0026lsquo;rounding\u0026rsquo; in a way.    We will create a similar binned time value in the net_stats_df by using the following:\n... net_stats_df.time_aligned = px.bin(net_stats_df.time_, 1000000000) ... Notice that we are using the same exact values for binning. This is important, as this is what will allow the dataframes to be aligned correctly.\nMerging The Two Tables Now that we have everything set up correctly, we can start merging (joining) the two tables. This is a similar step to the joining we have done before. Let\u0026rsquo;s add this snippet to the code, right before the px.display() call:\n... df = conn_stats_df.merge(net_stats_df, how=\u0026#39;left\u0026#39;, left_on=[\u0026#39;time_aligned\u0026#39;, \u0026#39;pod_id\u0026#39;], right_on=[\u0026#39;time_aligned\u0026#39;, \u0026#39;pod_id\u0026#39;], suffixes=[\u0026#39;\u0026#39;, \u0026#39;_x\u0026#39;]) ... This will result in one big table with all of the columns of conn_stats and network_stats. This includes duplicate columns (which will now have the suffix \u0026lsquo;_x\u0026rsquo; added on to them). Let\u0026rsquo;s take care of cleaning this data.\nAlso, take note that this is a left join, meaning that all the observations from the conn_stats table will be present.\nCleaning Up The Joined Table Let\u0026rsquo;s drop the columns that are duplicated:\n... df = df.drop([\u0026#39;time__x\u0026#39;, \u0026#39;time_aligned_x\u0026#39;, \u0026#39;pod_id_x\u0026#39;]) ... Now let\u0026rsquo;s rename some columns that used to be in network_stats for more clarity:\n... df[\u0026#39;received_bytes\u0026#39;] = df[\u0026#39;rx_bytes\u0026#39;] df[\u0026#39;received_packets\u0026#39;] = df[\u0026#39;rx_packets\u0026#39;] df[\u0026#39;received_errors\u0026#39;] = df[\u0026#39;rx_errors\u0026#39;] df[\u0026#39;received_drops\u0026#39;] = df[\u0026#39;rx_drops\u0026#39;] df[\u0026#39;transmitted_bytes\u0026#39;] = df[\u0026#39;tx_bytes\u0026#39;] df[\u0026#39;transmitted_packets\u0026#39;] = df[\u0026#39;tx_packets\u0026#39;] df[\u0026#39;transmitted_errors\u0026#39;] = df[\u0026#39;tx_errors\u0026#39;] df[\u0026#39;transmitted_drops\u0026#39;] = df[\u0026#39;tx_drops\u0026#39;] ... Of course, since we renamed the columns, we will have to drop the old columns:\n... df = df.drop([\u0026#39;rx_bytes\u0026#39;, \u0026#39;rx_packets\u0026#39;, \u0026#39;rx_errors\u0026#39;, \u0026#39;rx_drops\u0026#39;, \u0026#39;tx_bytes\u0026#39;, \u0026#39;tx_packets\u0026#39;, \u0026#39;tx_errors\u0026#39;, \u0026#39;tx_drops\u0026#39;]) ... Full Code You can find the full code for this script below. To give it a quick test-run, you can try it out using the \u0026lsquo;Scratch Pad\u0026rsquo; function of the Pixie Live UI.\nimport px # get the conn_stats data. conn_stats_df = px.DataFrame(\u0026#39;conn_stats\u0026#39;, start_time=\u0026#39;-5m\u0026#39;) # add contextual data about the cluster. conn_stats_df.pod = conn_stats_df.ctx[\u0026#39;pod\u0026#39;] conn_stats_df.pod_id = px.upid_to_pod_id(conn_stats_df.upid) conn_stats_df.container_name = px.upid_to_container_name(conn_stats_df.upid) conn_stats_df.container_id = px.upid_to_container_id(conn_stats_df.upid) conn_stats_df.namespace = px.pod_id_to_namespace(conn_stats_df.pod_id) conn_stats_df.node = px.pod_id_to_node_name(conn_stats_df.pod_id) # we convert the time value from nanoseconds and bin it to the nearest second (since there are 1000000000 ns in 1 s) conn_stats_df.time_aligned = px.bin(conn_stats_df.time_, 1000000000) # get the network_stats data. net_stats_df = px.DataFrame(\u0026#39;network_stats\u0026#39;, start_time=\u0026#39;-5m\u0026#39;) # we convert the time value from nanoseconds and bin it to the nearest second (since there are 1000000000 ns in 1 s) net_stats_df.time_aligned = px.bin(net_stats_df.time_, 1000000000) # merging the two dataframes based on the time_aligned and pod_id attributes. df = conn_stats_df.merge(net_stats_df, how=\u0026#39;left\u0026#39;, left_on=[\u0026#39;time_aligned\u0026#39;, \u0026#39;pod_id\u0026#39;], right_on=[\u0026#39;time_aligned\u0026#39;, \u0026#39;pod_id\u0026#39;], suffixes=[\u0026#39;\u0026#39;, \u0026#39;_x\u0026#39;]) # drop the duplicate time_ column df = df.drop([\u0026#39;time__x\u0026#39;, \u0026#39;time_aligned_x\u0026#39;, \u0026#39;pod_id_x\u0026#39;]) # rename some columns df[\u0026#39;received_bytes\u0026#39;] = df[\u0026#39;rx_bytes\u0026#39;] df[\u0026#39;received_packets\u0026#39;] = df[\u0026#39;rx_packets\u0026#39;] df[\u0026#39;received_errors\u0026#39;] = df[\u0026#39;rx_errors\u0026#39;] df[\u0026#39;received_drops\u0026#39;] = df[\u0026#39;rx_drops\u0026#39;] df[\u0026#39;transmitted_bytes\u0026#39;] = df[\u0026#39;tx_bytes\u0026#39;] df[\u0026#39;transmitted_packets\u0026#39;] = df[\u0026#39;tx_packets\u0026#39;] df[\u0026#39;transmitted_errors\u0026#39;] = df[\u0026#39;tx_errors\u0026#39;] df[\u0026#39;transmitted_drops\u0026#39;] = df[\u0026#39;tx_drops\u0026#39;] # get rid of the old named columns df = df.drop([\u0026#39;rx_bytes\u0026#39;, \u0026#39;rx_packets\u0026#39;, \u0026#39;rx_errors\u0026#39;, \u0026#39;rx_drops\u0026#39;, \u0026#39;tx_bytes\u0026#39;, \u0026#39;tx_packets\u0026#39;, \u0026#39;tx_errors\u0026#39;, \u0026#39;tx_drops\u0026#39;]) # display the merged dataframe px.display(df, \u0026#39;df\u0026#39;) Conclusion In this blog, we have understood how to create an advanced PXL script for use with clusters that have Pixie deployed on them. We have combined all the ideas from the previous blogs into a final, advanced tutorial script. You are now a PXL script expert! 🙌\nKeep in mind that there are a number of other functions you can use in your advanced PXL scripts. These can be found on Pixie\u0026rsquo;s documentation page.\n"
  },{
    "section": "Blog",
    "url": "https://intelops.ai/blog/how-to-create-a-frontend-website-using-nextjs/",
    "title": "Next.js 101 - Introduction and Tutorial",
    "description": "Get the basic idea of what Next.js is and create a simple webpage.",
    "searchKeyword": "",
    "content": " Next.js is a React framework maninly used to create web applications and by framework I mean it takes care of the tooling and the configurations that are required for React. It uses Server-Side Rendering(SSR). Okay now what is this SSR? it does exactly what its name suggests \u0026ldquo;renders on the server\u0026rdquo; which means you are basically creating a HTML file with the all the website\u0026rsquo;s content and then sending it to the user. SSR enables you to load pages faster even when you internet is slow, improves search engine optimization(SEO) and so on since we are not here to learn about server-side rendering. Now back to Next.js and why to use it.\n Why Next.js? Next.js is not the only framework that React has, one of their other popular framework is Gatsby. Now, comes the question of why you need to choose Next.js over the other frameworks. Though both Gatsby and Next.js are great on their on, for now lets just say that when I looked into both of them I found that Gatsby needs additional configurations that arent required in Next.js but incase you want to properly compare both of them I recommend Next.js vs Gatsby blog.\n Has page-based routing system (for dynamic routes). Optimized pre-fetching. Client-side routing. Pre-rendering and allows both static generation(SSG) and server-side rendering(SSR) for each page.  To create a website you need\u0026hellip;  NodeJs installed on your local  To install Node.js you can follow instructions from node.js installation\nCreating a webpage using Next.js Like any other framework Next.js also has its own command to setup a project. There are two ways in which you can do this.\n The first way is to open a terminal(command prompt) and type in the below line to start your project. This will ask you to for a project name.  yarn create next-app #or with npm: npx create Once that is done you\u0026rsquo;ll be able to access your project by running the commands below:\ncd your-project-name yarn dev #or npm run dev Now when you open the url http://localhost:3000 which will be visible on your terminal. Youl will be able to see a screen that might look something like this:\nThe second way is to manually create the project. Create a directory and then install the required dependencies using npm(Node Package Manager)  mkdir your-project-name #change directory to your project cd your-project-name Now add the package.json file in the root of your project directory\nyarn init #or npm init And finally to get started you\u0026rsquo;ll need Next, React and react-dom npm packages. To install these use the below command:\nyarn add next react react-dom #or npm install next react react-dom We\u0026rsquo;ll be following \u0026lsquo;process 1\u0026rsquo; for now. Let us create our first pafe now\nCreating your first page To see changes in the \u0026lsquo;localhost:3000\u0026rsquo; webpage. You need to make changes in index.js\nGo to [pages/index.js] Remove the existing code in the index.js file and try adding the code below:\nimport React from \u0026#34;react\u0026#34;; export default function FirstPage() { return ( \u0026lt;div\u0026gt; Page 1 \u0026lt;/div\u0026gt; ); } If you want to create another page just add another page into the pages folder. Ok, we have 2 pages but how do you connect and navigate between them? For this Nex.js has a special tag Link tag. All you have to is\nimport Link from \u0026#34;next/link\u0026#34;; And then connect the two pages by adding the following code to pages/index.js\nimport Link from \u0026#34;next/link\u0026#34;; export default function FirstPost() { return ( \u0026lt;h1 className=\u0026#34;title\u0026#34;\u0026gt; Go to\u0026lt;Link href=\u0026#34;/pages/newpage\u0026#34;\u0026gt;another page\u0026lt;/Link\u0026gt; \u0026lt;/h1\u0026gt; ); } But in this blog we are only concentrating on a single webpage. So let us add styles to our main page(index.js) for now.\nAdding styles to your pages We already have styles folder with globals.css. We will also need CSS modules - which add CSS at the component-level locally by creating unique classNames automatically.\nCreate a Layout component that can be used for all the pages:\n Create a components folder and inside it create layout.js and a CSS module called layout.module.css  In components/layout.module.css add the below code\n/* reference https://nextjs.org/learn/basics/assets-metadata-css/polishing-layout */ .container { max-width: 36rem; padding: 0 1rem; margin: 3rem auto 6rem; } .header { display: flex; flex-direction: column; align-items: center; } And in components/layout.js add the below code add:\nimport styles from \u0026#39;./layout.module.css\u0026#39;; export default function Layout({ children }) { return \u0026lt;div className={styles.container}\u0026gt;{children}\u0026lt;/div\u0026gt;; }  CSS modules are useful for component-level styles but if we want to style every page we can do that by adding styles to globals.css  Add the code below to styles/globals.css\n/* reference https://nextjs.org/learn/basics/assets-metadata-css/polishing-layout */ html, body { padding: 4; margin: 2; font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Oxygen, Ubuntu, Cantarell, Fira Sans, Droid Sans, Helvetica Neue, sans-serif; line-height: 1.6; font-size: 15px; } * { box-sizing: border-box; } a { color: #f39200; text-decoration: none; } a:hover { text-decoration: underline; } img { max-width: 100%; display: block; } To access the styling from globals.css you need to import them from pages/_app.js\nimport \u0026#39;../styles/global.css\u0026#39;; export default function App({ Component, pageProps }) { return \u0026lt;Component {...pageProps} /\u0026gt;; } As we are on it already lets create one last styling files to styles text in our webpage\nCreate a CSS file called styles/utils.module.css\n/* reference https://nextjs.org/learn/basics/assets-metadata-css/polishing-layout */ /* reference https://nextjs.org/learn/basics/assets-metadata-css/polishing-layout */ .heading2XL { font-size: 2.5rem; line-height: 1.2; font-weight: 800; letter-spacing: -0.05rem; margin: 1rem 0; } .headingXl { font-size: 2rem; line-height: 1.3; font-weight: 800; letter-spacing: -0.05rem; margin: 1rem 0; } .headingLg { font-size: 1.5rem; line-height: 1.4; margin: 1rem 0; } .headingMd { font-size: 1.2rem; line-height: 1.5; } .borderCircle { border-radius: 9999px; } .colorInherit { color: inherit; } .padding1px { padding-top: 1px; } .list { list-style: none; padding: 0; margin: 0; } .listItem { margin: 0 0 1.25rem; } .lightText { color: #9812e6; } Finally update components/layout.js and index.js\nimport Head from \u0026#34;next/head\u0026#34;; import Image from \u0026#34;next/image\u0026#34;; import styles from \u0026#34;./layout.module.css\u0026#34;; import utilStyles from \u0026#34;../styles/utils.module.css\u0026#34;; import Link from \u0026#34;next/link\u0026#34;; const name = \u0026#34;Your first webpage\u0026#34;; export const pageTitle = \u0026#34;Next.js Sample Webpage\u0026#34;; export default function Layout({ children, page }) { return ( \u0026lt;div className={styles.container}\u0026gt; \u0026lt;Head\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; href=\u0026#34;/favicon.ico\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;building a webpage with next.js\u0026#34; /\u0026gt; \u0026lt;/Head\u0026gt; \u0026lt;header className={styles.header}\u0026gt; {page ? ( \u0026lt;\u0026gt; \u0026lt;Image priority src=\u0026#34;/images/website.jpg\u0026#34; className={utilStyles.borderCircle} height={160} width={160} alt=\u0026#34;\u0026#34; /\u0026gt; \u0026lt;h1 className={utilStyles.heading2Xl}\u0026gt;{name}\u0026lt;/h1\u0026gt; \u0026lt;/\u0026gt; ) : ( \u0026lt;\u0026gt; \u0026lt;Link href=\u0026#34;/\u0026#34;\u0026gt; \u0026lt;Image priority src=\u0026#34;/images/website.jpg\u0026#34; className={utilStyles.borderCircle} height={100} width={100} alt=\u0026#34;\u0026#34; /\u0026gt; \u0026lt;/Link\u0026gt; \u0026lt;h2 className={utilStyles.headingLg}\u0026gt; \u0026lt;Link href=\u0026#34;/\u0026#34; className={utilStyles.colorInherit}\u0026gt; {name} \u0026lt;/Link\u0026gt; \u0026lt;/h2\u0026gt; \u0026lt;/\u0026gt; )} \u0026lt;/header\u0026gt; \u0026lt;main\u0026gt;{children}\u0026lt;/main\u0026gt; \u0026lt;/div\u0026gt; ); } In pages/index.js\nimport Head from \u0026#39;next/head\u0026#39;; import Layout, { siteTitle } from \u0026#39;../components/layout\u0026#39;; import utilStyles from \u0026#39;../styles/utils.module.css\u0026#39;; export default function Home() { return ( \u0026lt;Layout home\u0026gt; \u0026lt;Head\u0026gt; \u0026lt;title\u0026gt;{siteTitle}\u0026lt;/title\u0026gt; \u0026lt;/Head\u0026gt; \u0026lt;section className={utilStyles.headingMd}\u0026gt; \u0026lt;p\u0026gt;[Your Self Introduction]\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; (This is a sample website - you’ll be building a site like this on{\u0026#39; \u0026#39;} \u0026lt;a href=\u0026#34;https://nextjs.org/learn\u0026#34;\u0026gt;our Next.js tutorial\u0026lt;/a\u0026gt;.) \u0026lt;/p\u0026gt; \u0026lt;/section\u0026gt; \u0026lt;/Layout\u0026gt; ); } You should be able to see something like this at the end of it all: Conclusion In this blog, we covered how to install Next.js, creating a webpage and also styling it. To get a detailed explanation on each topic you can look into vercel\u0026rsquo;s official blog on Next.js. All I am trying to say is Next.js is a a great tool to create full-stack web applications and is easy to learn 😎, so what are you waiting for?\n"
  },{
    "section": "Blog",
    "url": "https://intelops.ai/blog/enhancing-your-pixie-pxl-script-by-manipulating-data/",
    "title": "Enhancing Your Pixie PXL Script By Manipulating Data",
    "description": "How to use in-built functionality of PXL to manipulate your data.",
    "searchKeyword": "",
    "content": " Pixie can be utilized for gathering monitoring metrics from your clusters. It offers you the benefit of having pre-written scripts, as well as custom scripts that extrapolate data. The custom scripts are written in Pixie\u0026rsquo;s very own language, PXL, which is similar to Python. Last week, we had been working on creating our first custom PXL script to gather data. In this tutorial, lets focus on customizing the data we have gathered and tuning it to our own preferences.\n PXL Uses DataFrames Within the PXL language, we can see that we use dataframes to interact with our data. For those of you familiar with Python, specifically pandas, this blog post will come as second nature to you. Dataframes are just tabular representations of data. You can think of a dataframe as a spreadsheet, but way more powerful.\nYou can tell from the script we wrote last week that the columns included were from the process_stats table. (see script below)\n# We import px, which is the library we will be using to add extra data to our table. import px # We gather data from the last 5 minutes, from the `process_stats` table, and create a dataframe from it. df = px.DataFrame(table=\u0026#39;process_stats\u0026#39;, start_time=\u0026#39;-5m\u0026#39;) # Below, we are adding extra data to our table, using `context` or `execution_time_functions` df.pod_id = df.ctx[\u0026#39;pod_id\u0026#39;] df.pod_name = px.upid_to_pod_name(df[\u0026#39;upid\u0026#39;]) df.pod_id = px.pod_name_to_pod_id(df[\u0026#39;pod_name\u0026#39;]) df.cmd = df.ctx[\u0026#39;cmdline\u0026#39;] df.pid = df.ctx[\u0026#39;pid\u0026#39;] df.container_name = df.ctx[\u0026#39;container_name\u0026#39;] df.container_id = df.ctx[\u0026#39;container_id\u0026#39;] # We group the dataframe based on certain attributes, and aggregate the data. df = df.groupby([\u0026#39;pid\u0026#39;, \u0026#39;cmd\u0026#39;, \u0026#39;upid\u0026#39;, \u0026#39;container_name\u0026#39;]).agg() # We display the dataframe. px.display(df, \u0026#39;processes_table\u0026#39;) This script used basic functions on the dataframe, such as adding new columns. It also used slightly more advanced functions such as the groupby function, and the aggregation function .agg().\nLet\u0026rsquo;s get right into how we can enhance our PXL scripts by manipulating data.\nJoining Tables Using PXL On top of just adding a few extra columns, we can also join two tables together based on common columns shared by the two tables. This process is called merging. Take a look at the code below for an example/explanation.\n# We import px, which is the library we will be using to add extra data to our table. import px # We gather data from the last 5 minutes, from the `conn_stats` table, # and create a dataframe from it. df = px.DataFrame(\u0026#39;conn_stats\u0026#39;, start_time=\u0026#39;-5m\u0026#39;) # We also gather data from the `http_events` table. http_e_df = px.DataFrame(\u0026#39;http_events\u0026#39;, start_time=\u0026#39;-5m\u0026#39;) # We can now combine the two tables, using the merge function. df = df.merge(http_e_df, how=\u0026#39;left\u0026#39;, left_on=[\u0026#39;time_\u0026#39;, \u0026#39;upid\u0026#39;], right_on=[\u0026#39;time_\u0026#39;, \u0026#39;upid\u0026#39;], suffixes=[\u0026#39;\u0026#39;, \u0026#39;_x\u0026#39;]) px.display(df, \u0026#39;conn_stats_and_http_events_table\u0026#39;) In the script above, we are using the merge function to join columns from the http_events table to the conn_stats table. Here is a brief explanation of what the parameters in this function mean:\n how: how we are going to be joining one table to another.\n'left' means we will keep all data from the left table.\n'right' means we keep all data from the right table.\n'inner' means we will only be keeping the data that is present in both tables. 'outer' means that we will be keep all data present in both tables. left_on/right_on: These define the columns which we will compare between the two tables to align the data correctly. In the code above, we are aligning data based on the time_ and upid columns. suffixes: defines what strings to attach to the duplicate columns in the resulting table. At the end of the merging done in this script, you will notice that we have columns from both tables. Yet, we will only have observations (rows) from the conn_stats table, since it is the left table.  Dropping Columns We can drop certain columns that we would not like from a table. For example, if there is a column that is duplicated from the previous merge we have done, we can drop it after merging. Take a look below:\n... # We can now combine the two tables, using the merge function. df = df.merge(http_e_df, how=\u0026#39;left\u0026#39;, left_on=[\u0026#39;time_\u0026#39;, \u0026#39;upid\u0026#39;], right_on=[\u0026#39;time_\u0026#39;, \u0026#39;upid\u0026#39;], suffixes=[\u0026#39;\u0026#39;, \u0026#39;_x\u0026#39;]) # we get rid of duplicate values such as `time__x` and `upid_x` df = df.drop([\u0026#39;time__x\u0026#39;, \u0026#39;upid_x\u0026#39;]) ... Notice that the colums we are dropping have the duplicate suffixes attached to their names. This ensures that the original columns are still present, so that we do not lose the data.\nAdding A Custom Column We can add custom columns to our data based on calculations we have done ourselves, or calculations based on other columns. This process is called mapping. For example, we might want to convert bytes to megabytes. This can be done via:\ndf[\u0026#39;req_body_size\u0026#39;] = df[\u0026#39;req_body_size\u0026#39;]/1.0e6 We can also add custom columns with whatever data we would like. If I wanted a column named foo, with the attribute bar added to each observation, I could do that using the following:\ndf[\u0026#39;foo\u0026#39;] = \u0026#34;bar\u0026#34; Filtering Data We can filter data within our script using PXL\u0026rsquo;s filter function. This functionality is similar to what is done in Python\u0026rsquo;s pandas package. In the example below, I am filtering to include the rows that have their bytes_sent value higher than 65399738:\ndf = df[df[\u0026#39;bytes_sent\u0026#39;] \u0026gt; 65399738] Other Useful Functions Pixie docs list a whole bunch of useful functions that can be applied to PXL dataframes. Some of my favorites are:\n Dataframe.head(): For when you need only a certain number of rows to be received from Pixie. This is extremely helpful in debugging while you are writing PXL scripts. Dataframe.groupby(): As we have used in our previous PXL blog. Dataframe.stream(): For when you have so much data that you need it on a streaming basis.  Conclusion In this blog, we have understood what PXL dataframes are, and the special dataframe functions we can use to enhance our PXL script and manipulate our data. Feel free to look at the PXL Docs to learn more.\n"
  },{
    "section": "Blog",
    "url": "https://intelops.ai/blog/how_beneficial_alpine_linux_docker_image_really_is/",
    "title": "Is Alpine Linux Docker Image for you?",
    "description": "Alpine is widely used to package projects into lightweight applications. It is one of the most used Docker official images based on Alpine Linux distribution. Let&#39;s learn how to use it.",
    "searchKeyword": "",
    "content": "Alpine is a minimal Docker image based on Alpine linux distribution and is only 5 MB in size. The image is built around musl libc and BusyBox and has access to a lot of package repositories. If you are hearing about it for the first time and want to learn how to use it, check this out.\nWhat makes alpine linux popular Using Alpine Linux as a base image in your Docker containers has its advantages, such as a smaller footprint and a reduced attack surface. It can also help with cost saving since deployments are faster, storage requirements are lower and network transfer costs are also lower.\nBUT WAIT!!! If everything is so good why is everyone not using it already? 🤔\nIt is usually a great choice to use if you are building and containerizing projects written in Go, Rust, C or C++. Which is probably a choice of language used in Creating Servers or Router-based networking applications. If you are a Data Scientist or Machine Learning Engineer thinking about using Alpine Linux as a base image to containerize your ML-model, I\u0026rsquo;m afraid it may do more bad than good to you since you are mostly using python and all the jazzed up packages that comes with it. 😵‍💫🔨\nWhy should you reconsider alpine linux for base image   Alpine linux uses musl libc instead of the more common glibc, what does that mean? Saying good bye to precompiled wheels for your popular machine learning libraries.\n  You can still use them by building them from source but that means increased build times, increased memory and can even cause inconsistent behavior.\n  Alpine linux packages are slower, which means they may need more compute resources compared to other distributions.\n  Some security tools may not properly scan Alpine Linux for security vulnerabilities as there is no complete databases of security issues for Alpine.\n  If this doesn\u0026rsquo;t convice then check this out, let\u0026rsquo;s pull latest Ubuntu image\n$docker pull ubuntu Using default tag: latest latest: Pulling from library/ubuntu Status: Downloaded newer image for ubuntu:latest w lets pull an Alpine image `powershell ocker pull alpine ing default tag: latest test: Pulling from library/alpine atus: Downloaded newer image for alpine:latest  checking their sizes we see alpine is clearly winning. `powershell ocker image ls POSITORY TAG IMAGE ID SIZE pine latest 9ed4aefc74f6 7.05MB untu latest 08d22c0ceb15 77.8MB w lets create a dockerfile to package a Python application that uses popular ML libraries [pandas](https://pypi.org/project/pandas/) and [scikit-learn](https://pypi.org/project/scikit-learn/). rst we will use Debian-based official Python image. `dockerfile OM python:3.8-slim N pip install --no-cache-dir scikit-learn pandas  No!! That build was so slow. It took me 32.9s to build that image and the size is a massive 420MB.🫤 build_image1](images/dockerbuild1.jpg) od for me I have alpine linux to the rescue, it's only 7.05MB it should finish the build in less than 5s.😎 `dockerfile OM python:3.8-alpine N pip install --no-cache-dir scikit-learn pandas at!!!😵 Build failed. build_image2](images/dockerbuild2.jpg) ay, not a problem it looks like the build failed because we dont have a precompiled wheel. Hate to do it but I told you so.😏 Let's install the dependencies first and then build the scikit-learn from source, that should be easy. Right? `dockerfile Stage 1: Install build dependencies and build wheels OM python:3.8-alpine as builder Update package list and install build dependencies N apk add --no-cache --virtual .build-deps g++ gcc gfortran musl-dev lapack-dev openblas-dev Create a directory to store wheels N mkdir /wheels Build wheels for scikit-learn and pandas N pip wheel --wheel-dir=/wheels --no-cache-dir scikit-learn pandas Stage 2: Install the wheels, remove build dependencies OM python:3.8-alpine Copy wheels from the builder stage PY --from=builder /wheels /wheels Install runtime dependencies N apk add --no-cache libstdc++ lapack openblas Install scikit-learn and pandas from wheels N pip install --no-cache-dir --find-links=/wheels scikit-learn pandas \u0026amp;\u0026amp; rm -rf /wheels ��‍💨 After a 102min long build I finally have my project image built using alpine linux. build_image3](images/dockerbuild3.jpg) t's see if I atleast get my tiny image as expected because I'm using Alpine linux as the base image to build. `powershell ocker image ls POSITORY TAG IMAGE ID CREATED SIZE pine_pandas_scikit latest da0d042b37b3 9 minutes ago 604MB ndas_scikit latest e24bb8d14cfa 3 hours ago 420MB ll...!!!😮‍💨 We all knew where it was heading. I'm not surprised. Are you? ## Conclusion ile Alpine Linux has its benefits, it may not be the best choice for every project, particularly those involving Python and it's Jazzed up libraries for Machine Learning. It is essential to evaluate specific requirements of your project and chose the most suitable base image accordingly.   "
  },{
    "section": "Blog",
    "url": "https://intelops.ai/blog/grpc-interceptors-using-go/",
    "title": "gRPC Interceptors using go",
    "description": "Implementation of gRPC interceptors in go",
    "searchKeyword": "",
    "content": " gRPC is an open-source, high-performance framework for developing remote procedure call (RPC) applications. It was created by Google and is now a part of the Cloud Native Computing Foundation (CNCF). Interceptor support is one of gRPC\u0026rsquo;s key features. In this blog, we will look at interceptors and how to use them in gRPC applications written in the Go programming language with monitoring example.\nWhy Interceptors in gRPC? Interceptors in gRPC are useful because they allow developers to add custom logic to the request/response processing pipeline. One exciting reason for the need for gRPC interceptors is that they allow us to implement cross-cutting concerns in a modular and reusable manner. Assume we want to add authentication to all of our gRPC services. Rather than modifying each service separately, we can write a single interceptor that checks the authentication token and adds the user ID to the request context. This interceptor can then be added to any gRPC service we create without requiring any changes to the service code. Another interesting reason for using gRPC interceptors is that they enable us to implement features like tracing and monitoring. We can easily trace the flow of requests through our system and identify any performance or reliability issues by including an interceptor that logs the start and end of each request. Similarly, by incorporating an interceptor that collects metrics on request/response sizes and latencies, we can monitor our system\u0026rsquo;s health and detect any anomalies.\nWhat are interceptors in gRPC? Interceptors are one of the powerful gRPC features that allows you to intercept and modify client-server requests and responses. Interceptors are middleware that sits between the client and server handlers, intercepting requests and responses as they traverse the network stack.\n Interceptors can ensure that only authorised users have access to resources through authentication and authorization. Interceptors can log request and response metadata to assist with debugging and performance analysis. Caching: By caching responses, interceptors can reduce network requests. Compression and encryption: By compressing and encrypting requests and responses, interceptors can improve performance and security.  Go implementation for interceptors in gRPC In this blog, We are going to create a logging middleware which helps to log the messages. Follow the below steps to achieve it.\nPre-requisite  Go (https://go.dev/doc/install) protoc (https://grpc.io/docs/protoc-installation/) Evans CLI (https://github.com/ktr0731/evans/releases) [It\u0026rsquo;s totally optional, I\u0026rsquo;ve used it to test my server]  Source Code Please refer the code here\nInstall the required modules from go package\ngo install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest Create a proto file with the messages and services required.\nsyntax = \u0026#34;proto3\u0026#34;;package demo;option go_package=\u0026#34;./;demo\u0026#34;;message DemoRequest { string message = 1;}message DemoResponse { string message = 1;}// gRPC service which has a Demo method returns message as response service MyService { rpc DemoMethod(DemoRequest) returns (DemoResponse) {}} There is only one method named \u0026ldquo;DemoMethod\u0026rdquo; in the service, which is defined in the \u0026ldquo;demo\u0026rdquo; package. A message of type \u0026ldquo;DemoRequest\u0026rdquo; is passed to the \u0026ldquo;DemoMethod\u0026rdquo; in order to receive it, and it returns a message of type \u0026ldquo;DemoResponse\u0026rdquo; in return. The messages include a single field called \u0026ldquo;message\u0026rdquo; that is of the string data type and has the tag value 1. The \u0026ldquo;rpc\u0026rdquo; keyword is used in the \u0026ldquo;MyService\u0026rdquo; service definition to denote remote procedure calls for client-server communication.  Compile this proto file using protoc with the following command:\nmkdir pb \u0026amp;\u0026amp; protoc --go_out=./pb --go-grpc_out=./pb proto/*.proto  Using Protocol Buffers files in the \u0026ldquo;proto\u0026rdquo; directory, this command creates a directory called \u0026ldquo;pb\u0026rdquo; and generates Go code for gRPC services and the messages that go along with them. The resulting code comprises message definitions, gRPC server and client stubs, and is stored in the \u0026ldquo;pb\u0026rdquo; directory.  Start building your server using Go as explained below. Open Terminal and type the following commands:\n# Please use your module name for the further references go mod init YourModuleNameGoesHere # This line will create a main.go file in root dir touch main.go Open main.go file and Try this code:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; pb \u0026#34;YourModuleNameGoesHere/pb\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; \u0026#34;google.golang.org/grpc/reflection\u0026#34; ) // gRPC loggingInterceptor which helps to log func loggingInterceptor(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) { log.Printf(\u0026#34;Received request: %v\u0026#34;, req) resp, err := handler(ctx, req) return resp, err } type Server struct { pb.UnimplementedMyServiceServer } func main() { // Create a new gRPC server with the logging interceptor \ts := grpc.NewServer( grpc.UnaryInterceptor(loggingInterceptor), ) // Register your gRPC service with the server \tmyService := \u0026amp;Server{} pb.RegisterMyServiceServer(s, myService) reflection.Register(s) // Listen on port 50051 \tlis, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:50051\u0026#34;) if err != nil { log.Fatalf(\u0026#34;failed to listen: %v\u0026#34;, err) } log.Printf(\u0026#34;Starting server in port :%d\\n\u0026#34;, 50051) // Start the server \tif err := s.Serve(lis); err != nil { log.Fatalf(\u0026#34;failed to serve: %v\u0026#34;, err) } }  We import the required packages, such as the gRPC framework from google.golang.org/grpc and our produced protobuf package pb, which defines our gRPC service and messages. To implement the MyServiceServer interface created by protobuf, we define a struct called Server. For every RPC method listed in our protobuf service description file, a method is present in this interface. The \u0026ldquo;DemoMethod\u0026rdquo; method, which accepts a Request message as input and outputs a Respond message, is what we implement. Here, we only send back a Respond message with a Message field that contains the Message field from the inbound Request message joined to \u0026ldquo;Hello\u0026rdquo;. To log incoming requests, we define the loggingInterceptor function. \u0026ldquo;grpc.UnaryServerInfo\u0026rdquo; object that includes information about the called RPC method is passed to this function together with a context object, the object representing the incoming request, and the context object. Also, it accepts a \u0026ldquo;grpc.UnaryHandler\u0026rdquo; object, which is a function that processes incoming requests and sends back responses. \u0026ldquo;grpc.NewServer()\u0026quot; is used to start a new gRPC server. With the command \u0026ldquo;grpc.UnaryInterceptor(loggingInterceptor)\u0026quot;, we pass in our loggingInterceptor function as a unary interceptor. With pb.RegisterMyServiceServer(s, myService), we tell the server about our Server struct. It instructs the gRPC server to use our Server struct to respond to requests for our service. We use net.Listen(\u0026ldquo;tcp\u0026rdquo;, \u0026ldquo;:50051\u0026rdquo;) to monitor port 50051 for incoming gRPC requests. s.Serve(lis) is used to launch the gRPC server. By doing this, an infinite loop is launched, which watches for incoming requests and processes them using our Server struct.  How to check this code\u0026hellip;\u0026hellip;??? Well, I have used Evans CLI, which is a development tool for creating and testing gRPC APIs. It includes an interactive shell with auto-completion and syntax highlighting, as well as the ability to automatically generate client and server stubs. It accelerates and shortens the development and testing of gRPC APIs. You can also test it by creating client side code. Just to make it simple, I\u0026rsquo;m using Evans CLI method.\nFirst, run the server using the following step in the terminal:\ngo run main.go Open a new terminal and run the evans cli which works like a client:\nevans -r repl -p 50051 # This command helps to choose the package  package demo # To check the services defined  show services :\u0026#39; # Output generated +-----------+------------+--------------+---------------+ | SERVICE | RPC | REQUEST TYPE | RESPONSE TYPE | +-----------+------------+--------------+---------------+ | MyService | DemoMethod | DemoRequest | DemoResponse | +-----------+------------+--------------+---------------+ \u0026#39; # To call this method use this command call DemoMethod :\u0026#39; # Output generated # Example 1 demo.MyService@127.0.0.1:50051\u0026gt; call DemoMethod message (TYPE_STRING) =\u0026gt; Dr.Strange { \u0026#34;message\u0026#34;: \u0026#34;Hello Dr.Strange\u0026#34; } # Example 2 demo.MyService@127.0.0.1:50051\u0026gt; call DemoMethod message (TYPE_STRING) =\u0026gt; Compage { \u0026#34;message\u0026#34;: \u0026#34;Hello Compage\u0026#34; } #logs generated from server 2023/03/31 23:37:40 Received request: message:\u0026#34;Dr.Strange\u0026#34; 2023/03/31 23:39:16 Received request: message:\u0026#34;Compage\u0026#34; \u0026#39; # To stop CLI use this command exit Conclusion Finally, we\u0026rsquo;ve seen how to use Go to implement an interceptor in a gRPC server. Interceptors enable us to extend the functionality of our gRPC server, such as logging or authentication, by intercepting incoming requests and modifying or performing additional actions on them before they are handled by our server. We saw how to define a server struct that implements the protobuf interface, how to define a logging interceptor function, and how to create a new gRPC server with the interceptor and register our service with it in this example. We also tested how it works with Evans CLI. Please feel free to add your suggestions.\n"
  },{
    "section": "Blog",
    "url": "https://intelops.ai/blog/writing-your-first-pixie-pxl-script/",
    "title": "Writing Your First PXL Script",
    "description": "How to write your own custom PXL script, for use with Pixie.",
    "searchKeyword": "",
    "content": " Pixie is an amazing tool used for monitoring Kubernetes clusters. It allows you to easily visualize different metrics about your cluster, all available from the easy-to-use Pixie Live UI. You might have noticed that different types of monitoring data from Pixie is retrieved using different scripts (located in the top left corner of the Live UI). The scripts included with Pixie are powerful, and allow access to multiple metrics, all presented in coherent visualizations. However, did you know that you can access the same information in tabular format, which you can manipulate to your benefit? These are known as PXL scripts and are used to communicate with Pixie. Let\u0026rsquo;s dig in to creating and testing your first custom PXL script.\n What Is A PXL Script? PXL scripts are used to get/manipulate the telemetry data gathered by Pixie. You can also use PXL scripts to collect data from new sources. They are written in a custom language developed by Pixie called PXL. PXL is similar to Python, in its syntax, and its use of dataframes (used by many pythonistas in the pandas library for data manipulation). These scripts can be executed by the Live UI, CLI, or even API. In our case, to keep this tutorial short and easy, let\u0026rsquo;s focus on executing PXL scripts on the Live UI.\nWriting Your First PXL Script All PXL scripts are text files which end in the file extension .pxl. So, let\u0026rsquo;s get started by creating a file: my_first_script.pxl. Within this file, paste the code below. I\u0026rsquo;ve added comments so you can follow along and understand what each line of the code does.\n# We import px, which is the library we will be using to add extra data to our table. import px # We gather data from the last 5 minutes, from the `process_stats` table, and create a dataframe from it. df = px.DataFrame(table=\u0026#39;process_stats\u0026#39;, start_time=\u0026#39;-5m\u0026#39;) # Below, we are adding extra data to our table, using `context` or `execution_time_functions` df.pod_id = df.ctx[\u0026#39;pod_id\u0026#39;] df.pod_name = px.upid_to_pod_name(df[\u0026#39;upid\u0026#39;]) df.pod_id = px.pod_name_to_pod_id(df[\u0026#39;pod_name\u0026#39;]) df.cmd = df.ctx[\u0026#39;cmdline\u0026#39;] df.pid = df.ctx[\u0026#39;pid\u0026#39;] df.container_name = df.ctx[\u0026#39;container_name\u0026#39;] df.container_id = df.ctx[\u0026#39;container_id\u0026#39;] # We group the dataframe based on certain attributes, and aggregate the data. df = df.groupby([\u0026#39;pid\u0026#39;, \u0026#39;cmd\u0026#39;, \u0026#39;upid\u0026#39;, \u0026#39;container_name\u0026#39;]).agg() # We display the dataframe. px.display(df, \u0026#39;processes_table\u0026#39;) Summary: This script will get the data from Pixie\u0026rsquo;s process_stats table from the past 5 minutes. The script will also add other context that is missing from the original table. Lastly, the script will group the data based on certain columns, and aggregate the data.\nNote: The px.display() function is required for the script to be able to run.\nTesting Your First PXL Script In order to test your PXL script, you can open up the Pixie LIVE UI, navigate to the top left corner, and click on the script button. Then, scroll up to the top where you will see the Scratch Pad option. This will allow you to paste your script into the editor that has opened up on the right-hand side. After pasting your script, you can hit the Run button in the top right corner to execute the script. You will then see the results table displayed in your Live UI.\nCommon Functions To Manipulate/Add To Your Data Pixie includes in-built execution-time functions that you can use to modify and/or manipulate your data. These can be viewed at the link here. Below, I have compiled a list of some of the most useful ones that I have encountered/used:\n px.upid_to_*(): Anything that starts with a upid_to_ is a great function, as it allows you to dervive context from the cluster you are working with. For example, px.upid_to_namespace() will get you the namespace that the current data is working under. This function is extremely useful, as every table you will be deriving data from (sans the network_stats table) contains a upid. px.time_to_int64(): Converts time to an Int64 value. Very useful for post-processing of time. px.vizier_name(): Gets the name/id of the cluster. px.atoi(): Converts a string value to an integer.  "
  },{
    "section": "Blog",
    "url": "https://intelops.ai/blog/use-crds-as-persistence-layer-for-applications/",
    "title": "Using Custom Resources (CRs) as Persistence Layer for applications in Kubernetes",
    "description": "Data persistence on Kubernetes for cloud native applications using Kubernetes CRs",
    "searchKeyword": "",
    "content": " We have been working on an open source project which runs on K8s cluster. We had a need to persist some data, we had two options. Either use a full-fledged database or store the data in some files in plain text format.\nWe came to know that the data can be persisted on K8s cluster in the form of K8s custom resources. As our solution runs on K8s so, we thought to leverage K8s custom resources as a persistence layer.\nIf you are new to K8s custom-resources, please read on below link.\n https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions.  The main difference between CR(custom resource) and CRD(custom resource definition) is that the CRD is a schema whereas CR represents a resource matching that CRD.\nSetup KinD based K8s cluster For this blog, let\u0026rsquo;s use KinD to create a local K8s cluster. Please follow steps given on below links.\n Install KinD from https://kind.sigs.k8s.io/docs/user/quick-start/#installing-from-release-binaries Create KinD cluster https://kind.sigs.k8s.io/docs/user/quick-start/#creating-a-cluster Check if you can access the cluster created in previous step, and you are able to list down the pods.  Code walkthrough All the code is present here. The project consists of ExpressJS with typescript as a choice of language. The structure of the repository looks like below,\n crds - containing the CRD (yaml specification) for the custom resource for which we have a REST CRUD operations implemented. src/store/k8s-client - containing the client code connecting to k8s and performing CRUD operations. other standard folders required for REST services implementation in ExpressJS.  The employee-crd.yaml looks like below.\napiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: # name must match the spec fields below, and be in the form: \u0026lt;plural\u0026gt;.\u0026lt;group\u0026gt; name: employees.intelops.ai spec: # group name to use for REST API: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt; group: intelops.ai names: # plural name to be used in the URL: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;plural\u0026gt; plural: employees # singular name to be used as an alias on the CLI and for display singular: employee # kind is normally the CamelCased singular type. Your resource manifests use this. kind: Employee # shortNames allow shorter string to match your resource on the CLI shortNames: - emp # either Namespaced or Cluster scope: Namespaced versions: - name: v1alpha1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: role: type: string required: [ \u0026#34;spec\u0026#34; ] Before we run the code, we have to apply the CRD onto the cluster like this.\nkubectl apply -f crds/employee-crd.yaml We can now create employee custom resources in the same fashion we do for in-built resources. In other words, all the kubectl commands which work with in-built resources, the same would work with your custom resources too.\nkubectl get employee -A Node client to K8s - @kubernetes/client-node To perform CRUD operations on the custom resources programmatically, we have used @kubernetes/client-node npm package. We have to initialise K8s client in the package and it\u0026rsquo;s done by adding below snippet in the code.\nimport * as k8s from \u0026#34;@kubernetes/client-node\u0026#34;; const kubeConfig = new k8s.KubeConfig(); if (process.env.NODE_ENV === \u0026#39;development\u0026#39;) { kubeConfig.loadFromDefault(); } else { kubeConfig.loadFromCluster(); } const client = kubeConfig.makeApiClient(k8s.CustomObjectsApi); Once the client is created using KubeConfig, we can use the methods available to perform actions on K8s resources( including custom resources).\nMethods to perform CRUD operations on K8s custom resource To create a custom resource, await client.createNamespacedCustomObject(group, version, namespace, plural, JSON.parse(payload)); To retrieve a custom resource, await client.getNamespacedCustomObject(group, version, namespace, plural, name); To update the given custom resource, Here the patch operation needs special treatment, the options needs a special header here like below\nconst options = {\u0026#34;headers\u0026#34;: {\u0026#34;Content-type\u0026#34;: k8s.PatchUtils.PATCH_FORMAT_JSON_PATCH}}; and the patch contains below\nconst patch = [{ \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec\u0026#34;, \u0026#34;value\u0026#34;: JSON.parse(payload) }]; The path contains the key which is getting replaced/patched.\nawait client.patchNamespacedCustomObject(group, version, namespace, plural, name, JSON.parse(patch), undefined, undefined, undefined, options); To list down all the custom resources, await client.listNamespacedCustomObject(group, version, namespace, plural, \u0026#34;true\u0026#34;, false, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, labelSelector); To delete the given custom resource, await client.deleteNamespacedCustomObject(group, version, namespace, plural, name) With above client code added, it can now be invoked from http handlers. You can check them here.\nRun the code cloned by following instructions given here. You can now use curl commands to create/update/read/delete custom resources.\nCreate an employee  Fire below command to create an employee.  curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;Mahendra\u0026#34;,\u0026#34;role\u0026#34;: \u0026#34;developer\u0026#34;}\u0026#39; http://localhost:8080/v1/employees  You can check if the employee was created by firing below command.  kubectl get employee -A Similarly, you can call other rest apis too which will retrieve the data from K8s.\nConclusion We saw that how we can use K8s custom resources to persist the application data when deploying a full-fledged database is not an option. Let us know your thoughts on this.\n"
  },{
    "section": "Blog",
    "url": "https://intelops.ai/blog/failed-pixie-deployment-on-civo-kubernetes/",
    "title": "Failed Pixie Deployment On Civo Kubernetes cloud? Here&#39;s How To Fix It",
    "description": "Pixie Observability tool&#39;s deployment failure on CIVO cloud.",
    "searchKeyword": "",
    "content": " Pixie is a great platform for monitoring telemetry data from your Kubernetes cluster. It provides both UI dashboards and a command-line interface. However, when trying to deploy Pixie on your Civo Kubernetes cluster, you might run into an error in which your Pixie deployment cannot gather data from your cluster. This results in an empty dashboard, and an error: Table 'http_events' not found. This post covers how to deploy Pixie on Civo Kubernetes and how to fix this error.\n Deploying Pixie On Civo Kubernetes In this section, in order to replicate the error shown above, we are going to be using helm to install Pixie onto our cluster. We do this by:\n Creating a sandbox cluster on Civo.  This cluster can have node pools of any size. However, Pixie recommends that each node have at least 1GB memory.   Create a deployment key on Pixie, under your profile\u0026rsquo;s Admin settings. Now that you have created your Civo cluster, you can deploy Pixie using helm.  Run the following commands to install Pixie via helm into the correct kubectl context:  # Add the Pixie operator chart. helm repo add pixie-operator https://pixie-operator-charts.storage.googleapis.com # Get latest information about Pixie chart. helm repo update # Install the Pixie chart (No OLM present on cluster). helm install pixie pixie-operator/pixie-operator-chart --set deployKey=\u0026lt;deploy-key-goes-here\u0026gt; --set clusterName=\u0026lt;cluster-name\u0026gt; --namespace pl --create-namespace   At this point, you will have deployed Pixie, only to see an error on the Pixie Live UI:\nTable 'http_events' not found\nFixing The Failed Deployment Here are a couple of things you can check for to make sure that Pixie can run on your Civo Kubernetes cluster:\n Check the kernel versions your nodes are running.  According to the official Pixie documentation, Pixie can only be deployed on nodes that run specific kernel versions. You can check the kernel version of your Civo cluster nodes by looking at Lens.  Follow the path below on Lens to see what your nodes are running: Homepage\u0026gt;Catalog\u0026gt;Clusters\u0026gt;{your_cluster_name}\u0026gt;Nodes From there, you can click on any node, and see which OS, OS Image, and Kernel version your node is running.     Check the Kubernetes version installed on your Civo cluster.  Civo installs a lightweight Kubernetes called k3s. The error we have created is present on k3s version v1.23.6-k3s1.  This actually ties into the previous kernel version error. K3s version v1.23.6-k3s1 uses Alpine Linux, which is not supported by Pixie at the moment.    We can bypass this by using the previous k3s version on our cluster, version v1.22.11-k3s1.  This older version of k3s uses Ubuntu, which is a distro supported by Pixie. This can be done by editing the config file used to create your cluster to reflect the older version of k3s:  kind: CivoKubernetes apiVersion: cluster.civo.crossplane.io/v1alpha1 metadata: name: sandbox spec: name: sandbox version: \u0026#34;1.22.11-k3s1\u0026#34;  Using this previous version of k3s, you can see on Lens that the correct OS, Ubuntu, is installed.       Now your Civo cluster will have the correct k3s version, as well as the correct OS version on each worker node. This should also allow you to see your data on Pixie correctly.\nConclusion In this blog, we covered how to deploy Pixie on a Civo Kubernetes cluster. We also covered how to fix the Table 'http_events' not found error, which causes your Pixie deployment to malfunction. The fix is as simple as downgrading the version of k3s we put on our cluster. All in all, Pixie is a great tool for monitoring telemetry data, and we hope that support for Alpine Linux gets added soon.\n"
  },{
    "section": "Blog",
    "url": "https://intelops.ai/blog/connecting-the-opentelemetry-collector-to-vecor/",
    "title": "Connecting The OpenTelemetry Collector To Vector",
    "description": "Series on how to transfer OpenTelemetry data to your desired Target using Vector.",
    "searchKeyword": "",
    "content": " OpenTelmetry provides amazing, standardized tools for collecting observability information about your cluster, one of which is a Collector which can receive/process/export data. Vector, on the other hand, is also a very powerful tool for collecting, transforming, and exporting data. Its power comes from the sheer amount of plugins available for it. To combine these two would result in an even more capable data pipeline, as all of your data would be in one place. However, if you try to export OpenTelemetry data to Vector via gRPC, you will likely encounter issues. This blog aims to show how to set up an OpenTelmetry Collector, set up Vector, and connect the two.\n Deploying An OpenTelemetry Collector We will be using helm\tto deploy our services to our cluster. This will allow for easy installs/uninstalls. It will also allow for being able to configure our services via one single YAML file each.\n Need to refresh your knowledge on YAML? Take a look at our blog Hacking YAML to your Benefit\n open-telemetry collector The OpenTelemetry Collector (OTel Collector) allows multiple sources including OTLP formatted data, Jaeger, and Prometheus. The data we will be passing into our OTel Collector will be Pixie data. If you would like a tutorial on how to deploy Pixie on your cluster, take a look at this blog.\nThere is a helm-chart that is offered by OpenTelemetry, which we will be using. Deploying the OTel Collector is as easy as running the following commands:\nhelm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts helm repo update helm install otel-collector open-telemetry/opentelemetry-collector This deployment will use the default values.yaml file. Download this file, rename it to collector_values.yaml, and keep in a folder, as we will be modifying it to redeploy the OTel Collector with our custom values.\nAt this point, you should set up your OTel Collector\u0026rsquo;s source correctly. In our case, this means updating the Pixie\u0026rsquo;s UI with the correct long-term data retention plugin (the OpenTelemetry one).\nDeploying A Vector Aggregator Deploying Vector is as easy as deploying the OTel Collector. The helm-chart can be found here, while the default values YAML file can be found here. Make sure to save this values file as well, as we will be updating it with our custom configuation, and redeploying Vector. The commands below will deploy the Vector Aggregator:\nhelm repo add vector https://helm.vector.dev helm repo update helm install vector vector/vector --namespace vector --create-namespace At any point, while Vector is deploying and running successfully, you can check the status of your aggregator\u0026rsquo;s events using this command:\nkubectl -n vector exec -it statefulset/vector -- vector top Modifying The Values Files Let\u0026rsquo;s start by opening up the collector_values.yaml file we had saved earlier. In this file, we need to change a few configurations for our OTel Collector to be able to communicate with our Vector Aggregator.\n The mode value needs to be changed from empty quotes to \u0026ldquo;deployment\u0026rdquo;. For this particular tutorial, using Pixie as the data source, the config parameter of this file needs to be modified to reflect Pixie as a data receiver: config: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 `` Within the same config parameter, we need to set the exporters. We will be exporting to two places, the logs for the OTel Collector pod, and the HTTP connection to Vector. exporters: logging: loglevel: debug sampling_initial: 5 sampling_thereafter: 200 otlphttp: endpoint: http://vector.vector.svc.cluster.local:80 `` Here, I have decided to use port 80 to send data to Vector. Within the config.service.pipelines parameter, we need to modify the following: pipelines: logs: { } metrics: receivers: [ otlp ] processors: [ memory_limiter, batch ] exporters: [ otlphttp, logging ] traces: receivers: [ otlp ] processors: [ memory_limiter, batch ] exporters: [ otlphttp, logging ] `` Your OTel Collector is ready to be upgraded. Use the following command to force upgrade your collector with the new values file. helm upgrade otel-collector open-telemetry/opentelemetry-collector -f collector_values.yaml `` Now, let\u0026rsquo;s move on to modifying the vector_values.yaml file to modfiy the configuration for our Vector Aggregator. The role parameter needs to be set to \u0026ldquo;Aggregator\u0026rdquo;. Within the service.ports parameter, we need to expose port 80 for our otlp (OpenTelemetry) data to flow in: ports: - name: otlp-http port: 80 protocol: TCP targetPort: 80 `` In the customConfig parameter, we provide our custom configuration for the Vector Aggregator we are going to deploy. customConfig: api: enabled: true address: 127.0.0.1:8686 playground: true sources: otlp-http: type: http address: 0.0.0.0:80 # this path is automatically added by OpenTelemetry. # this is because we are exporting metrics, so it adds a default path. # The path can be changed/set in the collector_values.yaml file. path: /v1/metrics encoding: text sinks: stdout: encoding: codec: json inputs: - otlp-http target: stdout type: console `` Here, we are receiving input (sources) from our OTel Collector, at localhost:80. This has a path /v1/metrics appended to it by the OTel Collector itself. For sinks (exporters) we are defining one exporter, standard output (stdout). This will take the data from our HTTP connection, and output it in the form of logs within our Vector Aggregator pod. Now you have configured all that is necessary for Vector to be able to get data from the OTel Collector. We can upgrade our Vector deployment with the new values, using the following command: helm upgrade vector vector/vector --namespace vector --values vector_values.yaml ``  Verifying Our Data Collection Using Vector At this point, with both the OTel Collector and the Vector Aggregator deployed, we should start seeing data flowing from one to the other. Run the command below to see how many events Vector has seen:\n# if on Windows, run in Admin PowerShell kubectl -n vector exec -it statefulset/vector -- vector top Which should give you an output similar to this: Horray! We have finally got the data flowing.\nConclusion In this blog, we have explored how to export data from an OpenTelemetry Collector to a Vector Aggregator. This is done using HTTP, as support for OpenTelemetry over gRPC has not been added yet to Vector. We hope that a solution gets created, but until then, feel free to use this workaround.\n"
  },{
    "section": "Blog",
    "url": "https://intelops.ai/blog/markdown-to-make-life-easy/",
    "title": "Markdown to Make Life Easy",
    "description": "How to use Markdown?",
    "searchKeyword": "",
    "content": " Markdown is an open-source markup language designed to convert plain text to HTML. Web writers widely use it to style and format their content in text editors for blogs and social media. Markdown\u0026rsquo;s popularity has recently risen due to its simple syntax and readable format. I\u0026rsquo;ll cover the fundamentals you\u0026rsquo;ll need to start writing your first Markdown file in this blog.\nHow to start writing your first Markdown file?  Let\u0026rsquo;s start by creating a new file on your favorite text editor (I like to use Sublime Text) and saving it as a \u0026ldquo;.md\u0026rdquo; file. Great!! Now that you have a markdown file created, you need to know a few basic syntaxes to start. To write a heading, use Hashmark (#). The number of hashmarks determines the level of the heading.   # Heading1\nHeading1 ## Heading2\nHeading2  ### Heading3\nHeading3 ####Heading4\nHeading4 ##### Heading5\nHeading5 ###### Heading6\nHeading6   A double-line break will create a new Paragraph.   This is paragraph 1This is paragraph 2\n  For font formatting, you can italicize the text by wrapping it with single asterisks or make it bold by wrapping it with double asterisks. *Want to make this Italic* **and this Bold**   Want to make this Italic and this Bold\n  Creating Hyperlinks is easy. Enclose a text inside square brackets and follow it with an URL inside parentheses as shown below. Click [Here](Your URL goes here) to learn about YAML   Click Here to learn about YAML\n  Greater than sign \u0026ldquo;\u0026gt;\u0026rdquo; is used to Blockquote the text or code. \u0026ldquo;\u0026gt;\u0026rdquo; can only be used at the beginning of a new line. \u0026ldquo;\u0026gt; Let\u0026rsquo;s BLOCKQUOTE this line\u0026rdquo; is rendered as below,   Let\u0026rsquo;s BLOCKQUOTE this line\n  Dash \u0026ldquo;-\u0026rdquo; or Number followed by a \u0026ldquo;.\u0026rdquo; can be used to make lists    item 1 item 2 item 3 OR   item 1 item 2 item 3    Great!! You are now set to start writing you content using Markdown.  What you learned so far was just the basics to get started. Using Markdown, you can also create Tables, Mathematical formulas, Flow charts, UML Diagrams, etc. Click Here To learn how to do more advanced stuff using Markdown.\nWhy should you use Markdown?  It\u0026rsquo;s Free!! You don\u0026rsquo;t have to buy paid software to build and publish your content. Easy and quick to create rich formatted content. You can use it with static site builders like Hugo to build your site in real-time and make changes along the way. Not just used to build websites. You can use it to create all types of content like notes, to-do lists, documentation, resumes, presentations, etc. Markdown is platform-independent, so you don\u0026rsquo;t have to worry about migrating the content across operating systems, Content Management Systems, etc. If you already know how to code in HTML, Markdown can be your magic wand since it has full HTML support. So, you can use HTML inline anywhere. If you know YAML(If you don\u0026rsquo;t know what YAML is click here) then you can use YAML for the Front Matter, page-specific metadata and functionality, at the top of your Markdown file.  Things to look out for  You cannot style your content freely and are limited to what you can change and what you cannot. Markdown is open source. There are so many flavors of Markdown that aren\u0026rsquo;t compatible with one another. No Standardization. So, you don\u0026rsquo;t know how it will render on a web browser. Markdown is a superset of HTML, making it a security risk. Adding HTML tags to Markdown can make it susceptible to XSS attacks. So use HTML tags cautiously.  Conclusion Markdown design to give fewer options to its users was by choice. Having more choices not only requires a lot of time to try them out but can also put a lot of users in a dilemma on deciding a better choice. So, at times having no option is a better option. Since Markdown has full HTML support, you can always add HTML tags anywhere you like to improve your content.\n"
  },{
    "section": "Blog",
    "url": "https://intelops.ai/blog/hacking-yaml-to-your-benefit/",
    "title": "Hacking YAML to your Benefit",
    "description": "Learn to use YAML.",
    "searchKeyword": "",
    "content": " YAML(yam-ul) is a Data Serialization language used to capture your data in a key-value pair format, making it easy to read and understand. In this article, I will cover the basic concepts of YAML. It\u0026rsquo;s Benefits and drawbacks over other markup languages like JSON and XML. YAML in DevOps and best practices to use while writing a YAML file.\n If you want to learn more about What is DevOps? Check this out.\n Basics to quickly start writing YAML files  A simple YAML file needs to have two items to be complete, a key and a value separated by a colon and a space.   ItemKey: ItemValue\n  Each line is a new item. You can have any number of items in a file.   Item1: ValueItem2: Value...Last_Item: Value\n  Users can add multiple documents into the same YAML file, just separate them using \u0026ldquo;\u0026mdash;\u0026rdquo;   --- ItemKey: ItemValue--- Item1: ValueItem2: Value...Last_Item: Value\n  Users can add a comment by starting it using a \u0026ldquo;#\u0026rdquo; symbol. If you have a multi-line comment, add \u0026ldquo;#\u0026rdquo; at the beginning of each line.   #Let\u0026rsquo;s create an item \u0026lsquo;a\u0026rsquo; with value 10a: 10#Now lets create another item \u0026lsquo;b\u0026rsquo; #with value 20 but wait create a thrid item \u0026lsquo;c\u0026rsquo; with value 30b: 20c: 30\n  We can create a List or a Dictionary in YAML.  List is a collection of elements defined by one key either as a block style,   States: - Texas - North Carolina - ArizonaOr you can use a flow styleStates: [Texas, North Carolina, Arizona]\n  Dictionary is a collection of key-value paired elements either as a block style,   Address: City: Raleigh State: North Carolina Zipcode: 27606Or you can use a Flow styleAddress: {City: Raleigh, State: North Carolina, Zipcode: 27606}\n  YAML supports all essential data types like nulls, numbers, and strings. It also recognizes a few language-specific data types, such as dates, timestamps, and unique numerical values.   String_item: \u0026ldquo;1234\u0026rdquo;Integer_item: 1234Float_item: 12.34Boolean_item: No\n Pros of using YAML over other markup languages  It has a human-readable format. So it is easy to understand. YAML, JSON, and XML store the data as Key-Value pairs. So converting a YAML file to JSON or XML is easy. Most languages use YAML for configuration files. More powerful to represent complex data. Due to its simple structure, parsing is easy. You can add comments to gain more insight into the data.  Cons of using YAML  It has a strict syntax, so we need to be cautious about the indentation. Even a single space mismatch can stop the code from working.   Easy way to overcome this would be to use a YAML validator.\n YAML is simple for writing but complex for processing since data can be represented in many ways making a complex data hierarchy.  YAML in DevOps YAML has seen recent popularity in the DevOps community due to all the benefits mentioned above. Creating a configuration file with configuration code that adheres to best practices is quick and easy. We can then define development pipelines using these configuration files. These pipelines are versioned, making it easy to find problems and undo changes with each new build.\nYaml configuration files are source files. So we add them to the root of the repository.\nYAML in Prominent DevOps tools:\n Azure DevOps: You can simplify defining the build and release tasks using a YAML designer provided by Azure. Kubernetes: Creating storage and lightweight Linux virtual machines using YAML Dockers: Feature YAML files called Dockerfiles that are blueprints for everything. To run the software, including codes, runtime, tools, settings, libraries, etc.  Conclusion Best Practices and things to look out for writing a config.yml file,\n Do not use a tab for indentation. Try to use the Flow style whenever you can to avoid creating complicated nested blocks. You cannot start a block-style list or dictionary on the same line as the mapping key   key: another_key: mapThis is not possibleKey: - List_itemThis is not possible too so try avoiding it.fruits: - apple: sweet lemon: sourThis is possible\n Save your YAML files with the \u0026ldquo;.yaml\u0026rdquo; or \u0026ldquo;.yml\u0026rdquo; extension.  "
  },{
    "section": "Blog",
    "url": "https://intelops.ai/blog/what-is-devsecops/",
    "title": "Adopting DevSecOps for development and testing - What and Why?",
    "description": "DevSecOps concept in development and testing.",
    "searchKeyword": "",
    "content": " Introduction to DevSecOps DevOps is a buzzword in the IT industry. With the rapid adoption of cloud, application development has become stronger, increasing the speed and agility of delivery processes. CI/CD results in faster time-to-market and improved stability. However, security is out of the picture.\nDevSecOps is a transformational shift that incorporates security culture, practices, and tools in each phase of the DevOps processes. It provides security right from the first stage and automates security tasks in the later stages of SDLC. The objective is to maximize security through minimizing potential errors or gaps that may be vulnerable.\nHow It Works How will a team know that it is delivering a secure application? To understand DevSecOps, all employees must take responsibility of the software’s security. Security is different from production. However, placing it as top priority may create friction between teams. This may happen due to lack of awareness. Right from the top management to developers, security must be integrated in daily tasks. Without a dedicated team of security professionals, it is hard to achieve speed and agility without the risk of important organizational data being compromised.\nCurrently, most organizations test for software vulnerabilities at the end of the SDLC, which is harmful. A robust and effective security approach incorporates security systems in planning, designing, and coding stages of automated testing.\n Verify: Production reviews are useful in identifying errors quickly. Data Visibility: Information about security threats or attacks should be shared across all departments to help everyone understand the potential vulnerabilities and act on it timely. If some part of the cord is broken, fix it immediately and send it to the CI/CD pipeline ensuring thereby that all tests are completed before delivering the fixed code.  Benefits DevSecOps reduces errors that often plague effective application development processes. By integrating security at the early stages of automation, it reduces the risks that can cause errors. Few benefits of DevSecOps include:\n Promotes automation: Security architects do not have to configure the test console manually. It results in improved efficiency, fewer errors, and faster production. Reduces Disputes: Security architects can make changes, adapt coverage, and increase process efficiency. Conflicts are reduced as problems are addressed in real time compared to changes made after the application is complete. It may be time consuming in the beginning, but with subsequent employee trainings, DevSecOps is easy to handle. Testing systems: Security testing at the end of SDLC may cause unexpected issues or conflicts interfering with the product functionalities. This may lead to more testing time, thereby causing production delays leading to higher running costs. With DevSecOps, automated systems can be tested in real time, resulting in faster repairs.  Conclusion It is hard to incorporate the mindset for DevSecOps. This organizational change requires a slow, deliberate approach. Implementing DevSecOps will create a collaborative environment where business stakeholders work with security architects and use appropriate tools for developing enterprise applications. There is no-one-size-fits all model.\nWith DevSecOps, enterprises can spend more time on strategic activities to add value to the customer rather than fixing security vulnerabilities in their application.\n Reference links to read more:-\n https://blog.fuelusergroup.org/an-introduction-to-devsecops https://www.coveros.com/introduction-devsecops https://safestack.io/blog/secure-development-introduction-to-devsecops   "
  },{
    "section": "Blog",
    "url": "https://intelops.ai/blog/what-is-data-breach/",
    "title": "What is data breach? 5 facts you need to know about data breach",
    "description": "Brief introduction on data breach.",
    "searchKeyword": "",
    "content": " Introduction As per Wikipedia, the term data breach refers to compromise of security involving unauthorized access to sensitive and confidential user information.  KPMG Survey - A triple threat across the Americas A recent survey by KPMG confirms the threat of fraud, cyber -attacks and compliance. 600+ senior leadership from seven industries across Latin and North America participated in the survey.\n 9 in 10 respondents say that remote working has reduced their ability to monitor behavior, thereby increasing the risks of external/internal fraud. 31% respondents also confirmed fraud perpetrated by an insider in the past year.\nRespondents also agreed that compliance has now become a reputational issue for enterprises. On an average, it takes about a month to fully contain a cyber-attack. there is a potentially fatal lack of urgency for most companies across the globe. not enough companies have championed fraud controls, compliance, and cybersecurity. it is also agreed that fraud, non-compliance, and cyber breaches cost dearly to the organization.\n   5 Facts About Data Breaches   A company takes almost six months to identify a data breach in the organization. Average total cost of a data breach is USD 4.35 million. Approximately 68 documents get compromised every second. In 2021, 98 percent of PoS data breaches in the hospitality industry were financial in nature. In 2022, top companies such as Uber, Samsung, Facebook, Toyota, Verizon, etc. have fallen a prey to data breaches.   Data breaches have increased over the recent years and affected enterprises of all sizes. The resultant damage of these data breaches costs several millions of dollars. For instance, T-mobile data breach cost $350 million. Hence, organizations should focus more on securing their networks; staff should have strong passwords and employees must be trained to identify the red flags.\n "
  },{
    "section": "Blog",
    "url": "https://intelops.ai/blog/devops-what-and-why/",
    "title": "DevOps - What and Why?",
    "description": "Brief concept intro on DevOps.",
    "searchKeyword": "",
    "content": " DevOps, literally, is the intersection of development and operations. It is an extremely broad term, used differently by different industries. The best definition for it is that it\u0026rsquo;s a combination of cultural philosophies, practices, and tools to release software fast and with high quality. This is accomplished by combining development, testing, security, and operations into a continuous and streamlined development/deployment process.\n Why Use DevOps? In order to recognize the benefits that DevOps brings to organizations, we must understand what DevOps is. The simplest way to understand DevOps is to consider it in the aspect of application development. Each time an application is created or updated, it follows a development process which ends with the app being deployed to the cloud or servers. The application is deployed in order to ensure delivery to its end-users. The application development process includes:\n Idea Requirements Coding/Testing Building/Packaging Deploying Operations \u0026amp; Monitoring  Each time an update happens to the application (i.e. a feature is added), you will need to run through the entire process. This would be fine, however, the process is riddled with hindrances which delay the release/deployment of new features. DevOps aims to improve on this process by making it faster, and delivering the same new features with minimal bugs.\n Hindrances in the Current Development Process The way the current process is set up, development and deployment (a.k.a. operations) are handled by two separate teams. This leads to obstacles which inhibit the process’s ability to implement new features, and introduces release delays. Some of which I have outlined below:\n Conflict of interests  Since development and operations are split into separate teams, this presents a conflict of interest between the two. The developers want to release as many new features as possible. The operations team wants to maintain the stability of their system. When new features are introduced, the stability of the system is risked, creating a conflict of interest. Ultimately, this will present itself as a release delay.   Lack of collaboration  With two separate teams, a lack of communication arises. Bad documentation or too many bugs can cause the operations team to send the application back to the developers. This back-and-forth ‘handover’ process adds slowdowns to the workflow. This can delay releases from days to months.   Manual work  Most of the release process contains manual work. Everything from deploying the app to configuring user access is done manually. Even if we consider creating scripts to automate parts of the process, those scripts would have to be run manually. Inter-peer and inter-team knowledge sharing becomes extremely cumbersome when doing manual work. This is because each person would have to update the other party on everything they have changed. Any changes in the server infrastructure can introduce massive delays and headaches in manual work.   Security  After development, a separate security team would check for vulnerabilities. This is also manual work, which, again, introduces delays in our application process (You might have heard of another intersectional term, DevSecOps, which incorporates the security aspect).   Testing  Most of the application tests are automated, however, given new features, certain aspects of the application need to be tested manually. This adds to the manual work and release delays.     By removing one hindrance at a time, DevOps aims to combine all of these different areas (development, testing, security, and operations). This creates an automated and streamlined development/deployment process. In tackling each hindrance, a series of best practices have manifested to help organizations to adopt DevOps culture.\n Best DevOps Practices In order for organizations to fully embrace DevOps philosophies, there are some key practices that need to be implemented. Starting with:\n Continuous Integration/Continuous Delivery pipeline - this application development and delivery pipeline can help organizations manage any operational challenges that might arise from transitioning to a DevOps culture. This pipeline also provides systematic and streamlined delivery of features/bug fixes. Microservices architecture - This type of application design allows for flexible application use, and easier innovation within each component of the application. Frequent updates - smaller, more frequent updates are key to the DevOps philosophy. These updates would usually be cumbersome, however, with all other practices embraced, can be easy to implement. Infrastructure As Code - this best practice helps organizations to maintain their development, testing, and production infrastructure through the use of code and development techniques. There are many other “X as Code” practices that have sprung up since this, such as Policy As Code. Collaboration - using tools associated with each DevOps practice, the responsibilities of the development and operations teams get merged. This increases collaboration between the teams resulting in knowledge sharing, and increased communication. Monitoring - by observing key metrics in real-time, organizations can maintain the stability of their applications. Organizations would also be monitoring the infrastructure that is serving the applications.   Benefits Of Implementing DevOps Upon implementing the practices, hindrances in the old development process are eliminated. The organization who has now adopted the DevOps culture can reap the benefits, which include:\n Reliability - systematic workflows help to ensure both the quality and integrity of the application. Delivery speed - the application can now have more frequent release, as well as there being less response time needed for bug-fixes. Improved collaboration between teams. Security - certain DevOps practices can include automated compliance policies which increase the security of your application. Scaling - through the use of certain practices, such as Infrastructure As Code, organizations can easily manage their development, testing, and production environments.   Conclusion DevOps is broad terminology referring to the set of practices which help release software with speed and quality. These practices were created to overcome hindrances in the old development process. By using a DevOps mindset, organizations can implement these practices, and ultimately the DevOps philosophy. Once embraced, this philosophy offers numerous benefits to the organization such as reliability, scaling, and reduced delivery speed.\n"
  },{
    "section": "Career",
    "url": "https://intelops.ai/career/digital-marketing-paid-media-manager/",
    "title": "Digital Marketing / Paid Media Manager",
    "description": "",
    "searchKeyword": "",
    "content": "What Is the role? We are looking for a personal financial planning pro (Certified Financial Planner™ preferred) who will lead our client advising efforts. You will be a fiduciary who works with clients providing holistic advice on areas including: saving, retirement, debt management, investments. You’ll be on the front lines helping clients in what is arguably the most important position at the company.\nWhy join Delta’s People team? We are especially interested in finding developers with experience either building web APIs with Go or experience with functional programming (e.g. Elixir, NodeJS, Clojure, F#). You might not have experience with all the technologies in our stack, but you are motivated to learn deeply. You will get the opportunity to work with both Go and Elixir with experienced team mates who can teach and pair with you to learn whatever you have less experience with.You care about security, code quality, scalability, performance, and simplicity. Above all, you seek operational excellence and apply the best engineering practices possible. Not everything that you or your team do can be perfect, but you make sure that you always know the trade-offs.\nSome of the high impact opportunities you’ll tackle?  must keep passwords secure and confidential; are solely responsible for User Data and all activity in their account while using the Service; must use commercially reasonable efforts to prevent unauthorized access to their account, and notify Conclude promptly of any such unauthorized access; and may use the Service only in accordance with Conclude\u0026rsquo;s online user guide and all applicable laws and regulations.  Responsibilities  Enhance or improve User experience, our Site, or our Service. Process transactions. Send emails about our Site or respond to inquiries. As this Privacy Policy and our Terms of Service.  Requirements We are looking for a personal financial planning pro (Certified Financial Planner™ preferred) who will lead our client advising efforts. You will be a fiduciary who works with clients providing holistic advice on areas including: saving, retirement, debt\nThis opportunity is for you if you have/are:  Enhance or improve User experience, our Site, or our Service. Process transactions requests for agreement. Send emails about our Site or respond to inquiries. Send emails and updates about Conclude, including news and requests for agreement to amended legal documents such. As this Privacy Policy and our Terms of Service.  "
  },{
    "section": "Career",
    "url": "https://intelops.ai/career/people-program-management-leader/",
    "title": "People Program Management Leader",
    "description": "",
    "searchKeyword": "",
    "content": "What Is the role? We are looking for a personal financial planning pro (Certified Financial Planner™ preferred) who will lead our client advising efforts. You will be a fiduciary who works with clients providing holistic advice on areas including: saving, retirement, debt management, investments. You’ll be on the front lines helping clients in what is arguably the most important position at the company.\nWhy join Delta’s People team? We are especially interested in finding developers with experience either building web APIs with Go or experience with functional programming (e.g. Elixir, NodeJS, Clojure, F#). You might not have experience with all the technologies in our stack, but you are motivated to learn deeply. You will get the opportunity to work with both Go and Elixir with experienced team mates who can teach and pair with you to learn whatever you have less experience with.You care about security, code quality, scalability, performance, and simplicity. Above all, you seek operational excellence and apply the best engineering practices possible. Not everything that you or your team do can be perfect, but you make sure that you always know the trade-offs.\nSome of the high impact opportunities you’ll tackle?  must keep passwords secure and confidential; are solely responsible for User Data and all activity in their account while using the Service; must use commercially reasonable efforts to prevent unauthorized access to their account, and notify Conclude promptly of any such unauthorized access; and may use the Service only in accordance with Conclude\u0026rsquo;s online user guide and all applicable laws and regulations.  Responsibilities  Enhance or improve User experience, our Site, or our Service. Process transactions. Send emails about our Site or respond to inquiries. As this Privacy Policy and our Terms of Service.  Requirements We are looking for a personal financial planning pro (Certified Financial Planner™ preferred) who will lead our client advising efforts. You will be a fiduciary who works with clients providing holistic advice on areas including: saving, retirement, debt\nThis opportunity is for you if you have/are:  Enhance or improve User experience, our Site, or our Service. Process transactions requests for agreement. Send emails about our Site or respond to inquiries. Send emails and updates about Conclude, including news and requests for agreement to amended legal documents such. As this Privacy Policy and our Terms of Service.  "
  },{
    "section": "Career",
    "url": "https://intelops.ai/career/senior-compensation-programs-manager/",
    "title": "Senior Compensation Programs Manager",
    "description": "",
    "searchKeyword": "",
    "content": "What Is the role? We are looking for a personal financial planning pro (Certified Financial Planner™ preferred) who will lead our client advising efforts. You will be a fiduciary who works with clients providing holistic advice on areas including: saving, retirement, debt management, investments. You’ll be on the front lines helping clients in what is arguably the most important position at the company.\nWhy join Delta’s People team? We are especially interested in finding developers with experience either building web APIs with Go or experience with functional programming (e.g. Elixir, NodeJS, Clojure, F#). You might not have experience with all the technologies in our stack, but you are motivated to learn deeply. You will get the opportunity to work with both Go and Elixir with experienced team mates who can teach and pair with you to learn whatever you have less experience with.You care about security, code quality, scalability, performance, and simplicity. Above all, you seek operational excellence and apply the best engineering practices possible. Not everything that you or your team do can be perfect, but you make sure that you always know the trade-offs.\nSome of the high impact opportunities you’ll tackle?  must keep passwords secure and confidential; are solely responsible for User Data and all activity in their account while using the Service; must use commercially reasonable efforts to prevent unauthorized access to their account, and notify Conclude promptly of any such unauthorized access; and may use the Service only in accordance with Conclude\u0026rsquo;s online user guide and all applicable laws and regulations.  Responsibilities  Enhance or improve User experience, our Site, or our Service. Process transactions. Send emails about our Site or respond to inquiries. As this Privacy Policy and our Terms of Service.  Requirements We are looking for a personal financial planning pro (Certified Financial Planner™ preferred) who will lead our client advising efforts. You will be a fiduciary who works with clients providing holistic advice on areas including: saving, retirement, debt\nThis opportunity is for you if you have/are:  Enhance or improve User experience, our Site, or our Service. Process transactions requests for agreement. Send emails about our Site or respond to inquiries. Send emails and updates about Conclude, including news and requests for agreement to amended legal documents such. As this Privacy Policy and our Terms of Service.  "
  },{
    "section": "Career",
    "url": "https://intelops.ai/career/executive-sr-executive-service-assurance/",
    "title": "Executive/ Sr. Executive - Service Assurance",
    "description": "",
    "searchKeyword": "",
    "content": "What Is the role? We are looking for a personal financial planning pro (Certified Financial Planner™ preferred) who will lead our client advising efforts. You will be a fiduciary who works with clients providing holistic advice on areas including: saving, retirement, debt management, investments. You’ll be on the front lines helping clients in what is arguably the most important position at the company.\nWhy join Delta’s People team? We are especially interested in finding developers with experience either building web APIs with Go or experience with functional programming (e.g. Elixir, NodeJS, Clojure, F#). You might not have experience with all the technologies in our stack, but you are motivated to learn deeply. You will get the opportunity to work with both Go and Elixir with experienced team mates who can teach and pair with you to learn whatever you have less experience with.You care about security, code quality, scalability, performance, and simplicity. Above all, you seek operational excellence and apply the best engineering practices possible. Not everything that you or your team do can be perfect, but you make sure that you always know the trade-offs.\nSome of the high impact opportunities you’ll tackle?  must keep passwords secure and confidential; are solely responsible for User Data and all activity in their account while using the Service; must use commercially reasonable efforts to prevent unauthorized access to their account, and notify Conclude promptly of any such unauthorized access; and may use the Service only in accordance with Conclude\u0026rsquo;s online user guide and all applicable laws and regulations.  Responsibilities  Enhance or improve User experience, our Site, or our Service. Process transactions. Send emails about our Site or respond to inquiries. As this Privacy Policy and our Terms of Service.  Requirements We are looking for a personal financial planning pro (Certified Financial Planner™ preferred) who will lead our client advising efforts. You will be a fiduciary who works with clients providing holistic advice on areas including: saving, retirement, debt\nThis opportunity is for you if you have/are:  Enhance or improve User experience, our Site, or our Service. Process transactions requests for agreement. Send emails about our Site or respond to inquiries. Send emails and updates about Conclude, including news and requests for agreement to amended legal documents such. As this Privacy Policy and our Terms of Service.  "
  },{
    "section": "Career",
    "url": "https://intelops.ai/career/programmer-web-developer/",
    "title": "Programmer/ Web Developer",
    "description": "",
    "searchKeyword": "",
    "content": "What Is the role? We are looking for a personal financial planning pro (Certified Financial Planner™ preferred) who will lead our client advising efforts. You will be a fiduciary who works with clients providing holistic advice on areas including: saving, retirement, debt management, investments. You’ll be on the front lines helping clients in what is arguably the most important position at the company.\nWhy join Delta’s People team? We are especially interested in finding developers with experience either building web APIs with Go or experience with functional programming (e.g. Elixir, NodeJS, Clojure, F#). You might not have experience with all the technologies in our stack, but you are motivated to learn deeply. You will get the opportunity to work with both Go and Elixir with experienced team mates who can teach and pair with you to learn whatever you have less experience with.You care about security, code quality, scalability, performance, and simplicity. Above all, you seek operational excellence and apply the best engineering practices possible. Not everything that you or your team do can be perfect, but you make sure that you always know the trade-offs.\nSome of the high impact opportunities you’ll tackle?  must keep passwords secure and confidential; are solely responsible for User Data and all activity in their account while using the Service; must use commercially reasonable efforts to prevent unauthorized access to their account, and notify Conclude promptly of any such unauthorized access; and may use the Service only in accordance with Conclude\u0026rsquo;s online user guide and all applicable laws and regulations.  Responsibilities  Enhance or improve User experience, our Site, or our Service. Process transactions. Send emails about our Site or respond to inquiries. As this Privacy Policy and our Terms of Service.  Requirements We are looking for a personal financial planning pro (Certified Financial Planner™ preferred) who will lead our client advising efforts. You will be a fiduciary who works with clients providing holistic advice on areas including: saving, retirement, debt\nThis opportunity is for you if you have/are:  Enhance or improve User experience, our Site, or our Service. Process transactions requests for agreement. Send emails about our Site or respond to inquiries. Send emails and updates about Conclude, including news and requests for agreement to amended legal documents such. As this Privacy Policy and our Terms of Service.  "
  },{
    "section": "Career",
    "url": "https://intelops.ai/career/technology-integration-support/",
    "title": "Technology Integration Support",
    "description": "",
    "searchKeyword": "",
    "content": "What Is the role? We are looking for a personal financial planning pro (Certified Financial Planner™ preferred) who will lead our client advising efforts. You will be a fiduciary who works with clients providing holistic advice on areas including: saving, retirement, debt management, investments. You’ll be on the front lines helping clients in what is arguably the most important position at the company.\nWhy join Delta’s People team? We are especially interested in finding developers with experience either building web APIs with Go or experience with functional programming (e.g. Elixir, NodeJS, Clojure, F#). You might not have experience with all the technologies in our stack, but you are motivated to learn deeply. You will get the opportunity to work with both Go and Elixir with experienced team mates who can teach and pair with you to learn whatever you have less experience with.You care about security, code quality, scalability, performance, and simplicity. Above all, you seek operational excellence and apply the best engineering practices possible. Not everything that you or your team do can be perfect, but you make sure that you always know the trade-offs.\nSome of the high impact opportunities you’ll tackle?  must keep passwords secure and confidential; are solely responsible for User Data and all activity in their account while using the Service; must use commercially reasonable efforts to prevent unauthorized access to their account, and notify Conclude promptly of any such unauthorized access; and may use the Service only in accordance with Conclude\u0026rsquo;s online user guide and all applicable laws and regulations.  Responsibilities  Enhance or improve User experience, our Site, or our Service. Process transactions. Send emails about our Site or respond to inquiries. As this Privacy Policy and our Terms of Service.  Requirements We are looking for a personal financial planning pro (Certified Financial Planner™ preferred) who will lead our client advising efforts. You will be a fiduciary who works with clients providing holistic advice on areas including: saving, retirement, debt\nThis opportunity is for you if you have/are:  Enhance or improve User experience, our Site, or our Service. Process transactions requests for agreement. Send emails about our Site or respond to inquiries. Send emails and updates about Conclude, including news and requests for agreement to amended legal documents such. As this Privacy Policy and our Terms of Service.  "
  },{
    "section": "Career",
    "url": "https://intelops.ai/career/asp-net-software-developer/",
    "title": "ASP.NET Software Developer",
    "description": "",
    "searchKeyword": "",
    "content": "What Is the role? We are looking for a personal financial planning pro (Certified Financial Planner™ preferred) who will lead our client advising efforts. You will be a fiduciary who works with clients providing holistic advice on areas including: saving, retirement, debt management, investments. You’ll be on the front lines helping clients in what is arguably the most important position at the company.\nWhy join Delta’s People team? We are especially interested in finding developers with experience either building web APIs with Go or experience with functional programming (e.g. Elixir, NodeJS, Clojure, F#). You might not have experience with all the technologies in our stack, but you are motivated to learn deeply. You will get the opportunity to work with both Go and Elixir with experienced team mates who can teach and pair with you to learn whatever you have less experience with.You care about security, code quality, scalability, performance, and simplicity. Above all, you seek operational excellence and apply the best engineering practices possible. Not everything that you or your team do can be perfect, but you make sure that you always know the trade-offs.\nSome of the high impact opportunities you’ll tackle?  must keep passwords secure and confidential; are solely responsible for User Data and all activity in their account while using the Service; must use commercially reasonable efforts to prevent unauthorized access to their account, and notify Conclude promptly of any such unauthorized access; and may use the Service only in accordance with Conclude\u0026rsquo;s online user guide and all applicable laws and regulations.  Responsibilities  Enhance or improve User experience, our Site, or our Service. Process transactions. Send emails about our Site or respond to inquiries. As this Privacy Policy and our Terms of Service.  Requirements We are looking for a personal financial planning pro (Certified Financial Planner™ preferred) who will lead our client advising efforts. You will be a fiduciary who works with clients providing holistic advice on areas including: saving, retirement, debt\nThis opportunity is for you if you have/are:  Enhance or improve User experience, our Site, or our Service. Process transactions requests for agreement. Send emails about our Site or respond to inquiries. Send emails and updates about Conclude, including news and requests for agreement to amended legal documents such. As this Privacy Policy and our Terms of Service.  "
  },{
    "section": "Learning center",
    "url": "https://intelops.ai/learning-center/learn-ebpf/",
    "title": "Learn eBPF",
    "description": "Learn sysflow eBPF Data Transfer",
    "searchKeyword": "",
    "content": ""
  },{
    "section": "Learning center",
    "url": "https://intelops.ai/learning-center/learn-ebpf/sysflow/",
    "title": "SysFlow",
    "description": "Learn sysflow eBPF Data Transfer",
    "searchKeyword": "",
    "content": ""
  },{
    "section": "Learning center",
    "url": "https://intelops.ai/learning-center/learn-ebpf/sysflow/sysflow-plugin/",
    "title": "SysFlow Plugin for ebpf data transfer",
    "description": "How to build a plugin for a sysflow to transfer eBPF data to your custom endpoint",
    "searchKeyword": "",
    "content": "How to build a plugin for a sysflow transfer eBPF data to your custom endpoint sf-processor provides a performance optimized policy engine for processing, enriching, filtering SysFlow events, generating alerts, and exporting the processed data to various targets.\nPlease check Sysflow Processor for documentation on deployment and configuration options.\n  Let\u0026rsquo;s clone the sf-processor repository.\ngit clone https://github.com/sysflow-telemetry/sf-processor.git `\n  Go to cloned repository\ncd sf-processor ``\n  Open the Dockerfile.\nvi Docker `` Add the local endpoint PORT to your Dockerfile\n\tEXPOSE 9091\t`` update loglevel=trace\n  Go to core/exporter/transports\ncd core/exporter/transports `` In file.go file find the Export() function. Add custom endpoint code\n\tresp, err := http.Post(\u0026quot;http://localhost:8080/api\u0026quot;, \u0026quot;application/json\u0026quot;, bytes.NewBuffer(buf)) if err != nil { return err } ``\n  In order to test in your local with docker container. Open sf-processor/docker-compose.yml file and add/update below fields under the sf-processor environment:\n\tPOLICYENGINE_MODE: enrich EXPORTER_TYPE: json EXPORTER_EXPORT: file EXPORTER_HOST: localhost EXPORTER_FILE_PATH: /processor-export/data.json # container local export data.json file path `` NOTE: Need to set ECS_TYPE_INFO = \u0026quot;trace\u0026quot; In order to see the trace logs in your sf-processor\n  Now build the docker build\ncd sf-processor make docker-build ``\n  Now log in to the public docker hub account in terminal or command line(CLI)\ndocker login -u username ``\n  Now rename the build docker image and push it to the your docker hub account.\nsudo docker images sudo docker tag sysflowtelemetry/sf-processor:0.5.0 \u0026lt;docker-hub-username\u0026gt;/sf-processor:0.5.0 sudo docker push \u0026lt;docker-hub-username\u0026gt;/sf-processor:0.5.0 ``\n  Sysflow deployment for a custom endpoint with docker hub image local testing sf-deployments contains deployment packages for SysFlow, including Docker, Helm, and OpenShift.\nPlease check Sysflow Deployments for documentation on deployment and configuration options.\n  Let\u0026rsquo;s clone the sf-deployments repository.\ngit clone https://github.com/sysflow-telemetry/sf-deployments.git `\n  Go to cloned repository\ncd sf-deployments ``\n  Open the docker config file.\nvi docker/config/.env.processor `` update below fields:\n\tPOLICYENGINE_MODE=enrich EXPORTER_FORMAT=json EXPORTER_EXPORT=file EXPORTER_FILE_PATH=/processor-export/data.json ``\n  Update the docker-compose.processor.yml file under the services -\u0026gt; sf-processer\nimage: \u0026lt;docker-hub-username\u0026gt;/sf-processer:0.5.0 example: image: pyswamy/sf-processor:0.5.0 `` under the Volumes:\nvolumes: - socket-vol:/sock/ - /tmp/sysflow:/processor-export/ ``\n  Now got to cd sf-deployment/docker/ do the deployment by running below command\nsudo docker-compose -f docker-compose.processor.yml up `` NOTE: The local api server is always up and running. https://localhost:8080/api\n  "
  },{
    "section": "Learning center",
    "url": "https://intelops.ai/learning-center/internal-guidelines/add-content-to-sites/",
    "title": "How to add content to website and docs sites",
    "description": "Steps to follow while adding content to blogs, learning-center and docs sites.",
    "searchKeyword": "",
    "content": "Everyone needs to follow those steps. Mandatory.\nPlease enhance the steps if necessary, but don\u0026rsquo;t do it wrong \u0026amp; complex. Make it easy for everyone.\nWatch the video "
  },{
    "section": "Learning center",
    "url": "https://intelops.ai/learning-center/learn-zenml/setup/",
    "title": "Setup",
    "description": "How to setup your local machine to use ZenML",
    "searchKeyword": "",
    "content": "How to make your system ZenML Ready ZenML comes as a Python library so it is necessary to have a python\u0026gt;=3.7,\u0026lt;= 3.10 installed on your local machine.\nVirtual environments let\u0026rsquo;s you have a stable, reproducible, and portable environment. You are in control of which package versions are installed and when they are upgraded.\nI use Anaconda to create and manage my Python envionments but you can also use pyenv-virtualenv or python -m venv.\n  Let\u0026rsquo;s create a new environment called zenml_playground.\nconda create --name zenml_playground python=3.8 `\n  Activate the virtual environment.\nconda activate zenml_playground ``\n  Install ZenML inside the virtual environment.\npip install zenml ``\n  [Optional] In order to get access to the ZenML dashboard locally you need to launch ZenML Server and Dashboard locally. For this, you need to install ZenML Server separately.\npip install \u0026#34;zenml[server]\u0026#34; ``\n  To verify if the installation is completed start Python interpreter and try to import zenml.\nimport zenml print(zenml.__version__) If you see a ZenML version displayed on your command prompt then you are all set to explore ZenML Steps and Pipelines.\n  "
  },{
    "section": "Learning center",
    "url": "https://intelops.ai/learning-center/learn-zenml/",
    "title": "Learn ZenML",
    "description": "Learn ZenML to make your Machine Learning model production ready.",
    "searchKeyword": "",
    "content": ""
  },{
    "section": "Learning center",
    "url": "https://intelops.ai/learning-center/learn-zenml/zenml-and-its-components/",
    "title": "ZenML and Its Components",
    "description": "What is ZenML and what are its Components",
    "searchKeyword": "",
    "content": "What is ZenML? It is a cloud and tool-agnostic open-source MLOPS framework that can be used to create a portable, production ready MLOps pipelines. It consists of following core components that you would need to know to get started.\n Steps Pipelines Stack Stack Components  What is a ZenML Step? Step is an atomic components of a ZenML Pipeline. Each Step is well defined to take some input, apply some logic on it and give an output. An example of a simple step could be as follows:\nfrom zenml.steps import step, Output @step def step_one() -\u0026gt; Output(output_a=int, output_b=int): \u0026quot;\u0026quot;\u0026quot;This Step returns a predefined values for a and b\u0026quot;\u0026quot;\u0026quot; return 5, 12 Let\u0026rsquo;s define another step that takes two values as input and returns a sum as output.\nfrom zenml.steps import step, Output @step def step_two(input_a: int, input_b: int) -\u0026gt; Output(output_sum=int): \u0026quot;\u0026quot;\u0026quot;Step that add the inputs and returns a sum\u0026quot;\u0026quot;\u0026quot; return input_a + input_b  Note:\nYou can run a step function by itself by calling .entrypoint() method with the same input parameters. For example: step_two.entrypoint(input_a = 6, input_b = 10)  What is a ZenML Pipeline? A Pipeline consists of a series of Steps, organized in any order as per your usecase. It is used to simply route the outputs through the steps. For example:\nfrom zenml.pipelines import pipeline @pipeline def pipeline_one(step_1, step_2 ): output_a, output_b = step_one() output_sum = step_two(output_a, output_b) After you define your pipeline you can instantiate and run your pipeline by calling:\npipeline_one(step_1 = step_one(), step_2 = step_two()).run() You should see an output similar to this in your command line:\nCreating run for pipeline: `pipeline_one` Cache disabled for pipeline `pipeline_one` Using stack `default` to run pipeline `pipeline_one` Step `step_one` has started. Step `step_one` has finished in 0.010s. Step `step_two` has started. Step `step_two` has finished in 0.012s. Pipeline run `pipeline_one-20_Feb_23-13_11_20_456832` has finished in 0.152s. You can learn more about pipelines here.\nWhat is a ZenML Stack? A stack is a set of configurations for your infrastructure on how to run your pipeline. For example if you want to run your pipeline locally or on a cloud. ZenML uses a default stack that runs your pipeline and stores the artifacts locally, if nothing is defined by the user.\nWhat are the Components of a Stack? A Stack Component is responsible for one specific task of an ML workflow. Consists mainly of two main groups:\n Orchestrator, responsible for the execution of the steps within the pipeline. Artifact Store, responsible for storing the artifacts generated by the pipeline.  Remember, for any of the stack components you need to first register the stack component to the respective component group and then set the registered stack as active to use it in the current run. For example if you want to use an S3 bucket as your artifact storage, then you need to first register the S3 bucket with the artifact-store with a stack name and then set the stack name as active. You can learn more about how to do this from here.\n"
  },{
    "section": "Learning center",
    "url": "https://intelops.ai/learning-center/internal-guidelines/",
    "title": "Internal Guidelines",
    "description": "Standards, Processes, etc. to follow, for internal/external engineering Teams",
    "searchKeyword": "",
    "content": ""
  },{
    "section": "Learning center",
    "url": "https://intelops.ai/learning-center/learn-zenml/run-zenml-pipeline/",
    "title": "Run ZenML Pipeline",
    "description": "How to create and run your ZenML Pipeline",
    "searchKeyword": "",
    "content": "So far we have seen,\n1. How to setup your local machine to use ZenML 2. What are the components of ZenML. 3. How to create ZenML Steps and Pipelines. 4. How to register different stacks to the registry and activate them to use for our project. Now, Let\u0026rsquo;s see what are some of the things that you need to keep in mind when converting your ML project into different Steps and Pipeine.\n You can probably create a step for each function within your ML model but commonly we try to group functions that do a certain task as a step. A common template to follow would be build a step for each stage of your model building. For example,  Data Ingestion Data Processing Data Splitting Model Training Model Evaluation Model Development Monitor Model Performance   Each step should be considered as its very own process that reads and writes its inputs and outputs from and to the artifact store. This is where materializers comes into play. A materializer dictates how a given artifact can be written to and retrieved from the artifact store. It contains all serialization and deserialization logic. You can know more about the materializers from here. Step and pipeline configurations are used to dynamically set parameters at runtime. It is a good practice to configure from the CLI and a YAML config:  Do this when you want to launch pipeline runs without modifying the code at all. This is most useful in production scenarios. Learn more from here.\n  You can create a separate step_filename.py for each step you like to use in your pipeline, especially if you want to create common steps that can be used across different pipelines. Repeat the same for materializers and store them as materializer_filename.py Now you can import any step and materializer that you want and create a pipeline using them. Save it as zenml_pipeline_filename.py Congrats!!! You are now all set to run your first pipeline.  python zenml_pipeline_filename.py [-c] [path/to/your/config_file.py] Yes it is as simple as that.\n"
  },{
    "section": "",
    "url": "https://intelops.ai/open-source/",
    "title": "Open-sources",
    "description": "",
    "searchKeyword": "",
    "content": ""
  },{
    "section": "Open source",
    "url": "https://intelops.ai/contributed-open-source/",
    "title": "Contributed Open Source",
    "description": "It’s been a bloody long ride, but Cinderblock has officially launched!",
    "searchKeyword": "",
    "content": ""
  },{
    "section": "Open source",
    "url": "https://intelops.ai/our-open-source/",
    "title": "Our Open Source",
    "description": "It’s been a bloody long ride, but Cinderblock has officially launched!",
    "searchKeyword": "",
    "content": ""
  },{
    "section": "",
    "url": "https://intelops.ai/platform/",
    "title": "Platforms",
    "description": "",
    "searchKeyword": "",
    "content": ""
  },{
    "section": "Platform",
    "url": "https://intelops.ai/capten/",
    "title": "Capten",
    "description": "It’s been a bloody long ride, but Cinderblock has officially launched!",
    "searchKeyword": "",
    "content": ""
  },{
    "section": "Platform",
    "url": "https://intelops.ai/compage/",
    "title": "Compage",
    "description": "It’s been a bloody long ride, but Cinderblock has officially launched!",
    "searchKeyword": "",
    "content": ""
  },{
    "section": "Platform",
    "url": "https://intelops.ai/opty/",
    "title": "Opty",
    "description": "It’s been a bloody long ride, but Cinderblock has officially launched!",
    "searchKeyword": "",
    "content": ""
  }]